{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Assignment 5: Text-based Industry Classification using Doc2Vec\n",
    "\n",
    "#### Group 1: Tara Bode and Hankun Li\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Specifics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the business sections (/blue/acg7849/share/BS) using Doc2Vec (50 clusters) in two ways:\n",
    "\n",
    "- Using a counter as the ‘tag’ (as in 5.1.5)\n",
    "- Using a counter as the ‘tag’, and the industry code as an additional tag (yield TaggedDocument(words=file_tokens, tags=[i, SIC]) where SIC is a string holding the tag (for example ‘1740’)\n",
    "\n",
    "Extract the 4-digit SIC industry code from the annual report header (STANDARD INDUSTRIAL CLASSIFICATION).\n",
    "\n",
    "Required: Evaluate whether adding the industry code as an additional tag improves the clustering. Use the standard deviation of profitability as a way to evaluate this. (Firms that are more similar, should have similar performance. Therefore, a better clustering would result in lower standard deviations for each cluster, relative to a worse clustering).\n",
    "\n",
    "Do this for the filings for the year 2019 only. Calculate the standard deviation of performance for each cluster (use the year of CONFORMED END OF PERIOD, which are the first 4 digits of ‘date’ in summary.text).\n",
    "\n",
    "For 50 clusters that means you will have 2 standard deviations for each cluster (one for each approach, with the extra SIC tag vs not adding the extra SIC tag). Use a t-test to test for a difference between the two sets of 50 standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/jupyterhub/1.1.0/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# all imports\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import html, re\n",
    "from w3lib.html import replace_entities\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from scipy.cluster import  hierarchy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "stopWords = set(stopwords.words('english') )\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some punctuation to string.punctuation\n",
    "punc = string.punctuation + '“”'\n",
    "\n",
    "# documents get tagged by an index (number), while filenames have different numbers\n",
    "# keep track of this\n",
    "fileIdToIndex = {} # given a fileId -> tag\n",
    "indexToFileId=[] # given a tag -> fileId\n",
    "\n",
    "class MyFiles(object):\n",
    "    def __init__(self, dirname, tokens_only = False):\n",
    "        self.dirname = dirname\n",
    "        self.tokens_only = tokens_only\n",
    " \n",
    "    def __iter__(self):\n",
    "        #for i, fname in enumerate(files[0:200]):\n",
    "         \n",
    "        for i, fname in enumerate(os.listdir(self.dirname)[0:4604]):    # there are 4604 files in total\n",
    "        # enumerate = return a list of tuples, iterate from start to end\n",
    "        # os.listdir = return index of a directory, input = directory address\n",
    "        # this part enumerates the first 200 units in the index under dirname\n",
    "        \n",
    "            with open( os.path.join(self.dirname, fname), encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            # filter\n",
    "                #content = [f for f in content if int(f[\"length\"])> 1000 and (f[\"date\"][0:4]) == '2019']\n",
    "                #content = [f for f in content if len(f)> 1000]\n",
    "            # grab id from filename\n",
    "            myCounter = int ( re.findall(r'(\\d*)\\.txt', fname)[0] )\n",
    "            # update \n",
    "            fileIdToIndex [ myCounter] = i\n",
    "            indexToFileId.append( myCounter)\n",
    "            #print('fname', fname, 'tag', myCounter)\n",
    "            file_tokens = [x for x in word_tokenize(content) if x.isalpha() and x.lower() not in stopWords and x not in string.punctuation]\n",
    "            \n",
    "            if self.tokens_only == True:\n",
    "                yield file_tokens\n",
    "            else:\n",
    "                yield TaggedDocument(words=file_tokens, tags=[i] )                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hipergator\n",
    "ffiles = MyFiles(r'/blue/acg7849/hli1/item1') # this one expected str, bytes or os.PathLike object\n",
    "#ffiles = MyFiles(r'/blue/acg7849/share/BS/item1/') # a memory-friendly iterator\n",
    "# dirname = '/blue/acg7849/share/BS/item1/'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model, build vocabulary\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=2, epochs=10)\n",
    "model.build_vocab(ffiles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISH\n"
     ]
    }
   ],
   "source": [
    "# train it\n",
    "model.train(ffiles, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "print('FINISH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hipergator\n",
    "def tokenizeFile(file_id):\n",
    "    with open( r'/blue/acg7849/hli1/item1/'+str(file_id)+'.txt', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    return ([x for x in word_tokenize(content) if x.isalpha() and x.lower() not in stopWords and x not in string.punctuation] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tokenizeFile(277350) # test file = 277350\n",
    "#model.infer_vector( t )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3711, 0.9844865798950195),\n",
       " (2789, 0.6870694160461426),\n",
       " (3302, 0.6644330620765686),\n",
       " (3647, 0.652461588382721),\n",
       " (26, 0.6479239463806152)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizeFile(277350)\n",
    "inferred_vector = model.infer_vector( t )\n",
    "# dv is short for docvecs\n",
    "# most similar files to 277350 is the 4th one 265065?\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "#sims = model.dv.most_similar([inferred_vector], topn=4)\n",
    "sims[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-0bceb90a8426>:2: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  similar_doc = model.docvecs.most_similar(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(4321, 0.7653406262397766),\n",
       " (1806, 0.7328099608421326),\n",
       " (2554, 0.727522075176239),\n",
       " (4225, 0.7245810031890869),\n",
       " (3588, 0.7169573307037354),\n",
       " (1656, 0.716842532157898),\n",
       " (4586, 0.7161878347396851),\n",
       " (3621, 0.7130987644195557),\n",
       " (324, 0.7120179533958435),\n",
       " (103, 0.7078105211257935)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# letter with filename 1.txt is the first letter, so tag is 0\n",
    "similar_doc = model.docvecs.most_similar(0)\n",
    "similar_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents 4604\n",
      "model.docvecs 4604\n"
     ]
    }
   ],
   "source": [
    "print('number of documents', model.corpus_count)\n",
    "print('model.docvecs', len(model.dv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.13251027,  1.7713019 , -1.2070944 ,  0.5877362 ,  3.2593195 ,\n",
       "        2.5406146 , -4.9172263 , -0.6247052 , -0.3348404 ,  0.6904445 ,\n",
       "        3.0260167 ,  1.055489  , -2.2696917 , -2.0256064 , -1.3583243 ,\n",
       "       -1.1576496 ,  0.74752647, -0.47880083, -2.6639798 , -2.8420856 ,\n",
       "       -0.9892699 ,  2.11443   , -2.0620131 ,  0.5325125 ,  1.0371944 ,\n",
       "       -0.02256046, -5.4824047 ,  0.52058345, -0.01334684, -0.41951287,\n",
       "        1.436628  ,  0.77188253, -0.9873223 , -0.2342852 , -1.8224778 ,\n",
       "       -0.83203405,  0.36926478, -0.44015148,  1.9534036 ,  1.2269791 ,\n",
       "       -1.4412913 , -1.4802396 ,  2.219955  ,  1.4758697 ,  1.6850762 ,\n",
       "       -4.326427  , -0.48489276, -1.3180994 ,  1.9214945 ,  0.01527684,\n",
       "        0.09991759,  1.133366  , -3.1863654 , -0.4483956 , -1.5857337 ,\n",
       "       -1.8974037 ,  1.2719195 ,  2.1387637 ,  0.3302084 ,  1.0655164 ,\n",
       "        0.43528807,  2.2369564 , -0.1382547 , -0.45128253,  0.6500001 ,\n",
       "       -0.6549607 ,  0.8065622 , -0.05199064, -2.0942473 , -4.1209855 ,\n",
       "       -0.997949  ,  1.4097698 ,  0.48933327, -0.09295246, -2.0775783 ,\n",
       "       -0.36108232,  0.83818686, -2.2805414 , -0.53319794,  0.6993395 ,\n",
       "        1.2344687 ,  0.7191078 , -1.0173111 , -1.5576121 ,  0.09775043,\n",
       "        0.08539424, -0.41517282,  1.2580737 , -1.1472104 ,  2.9920845 ,\n",
       "       -1.1958766 ,  3.0817316 , -2.032265  , -0.51527447, -3.1653166 ,\n",
       "        1.6203022 ,  2.2170432 , -1.55465   ,  5.788068  , -3.9377217 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reread the files, and get the vector for each file\n",
    "# feed vector into k-means algorithm to make clusters\n",
    "wordLists = MyFiles(r'/blue/acg7849/hli1/item1', tokens_only = True) # a memory-friendly iterator\n",
    "vectors = [ model.infer_vector( w ) for w in wordLists]\n",
    "print(len(vectors))\n",
    "vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 1: Using a Counter as the Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36, 21, 43, 24, 1, 14, 5, 29, 43, 9]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "num_clusters = 50\n",
    "kclusterer = KMeansClusterer(num_clusters, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "\n",
    "#assigned_clusters: sequence of files and matching clusters\n",
    "assigned_clusters = kclusterer.cluster(vectors, assign_clusters=True)\n",
    "assigned_clusters[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({33: 311, 12: 244, 24: 218, 37: 155, 45: 151, 27: 139, 42: 127, 23: 121, 46: 117, 40: 114, 49: 114, 41: 114, 29: 110, 34: 106, 7: 105, 36: 103, 4: 99, 9: 98, 3: 98, 22: 94, 5: 93, 39: 86, 20: 85, 35: 83, 0: 79, 13: 75, 43: 74, 28: 74, 15: 72, 11: 71, 21: 69, 44: 69, 48: 66, 30: 65, 31: 64, 38: 64, 19: 62, 47: 61, 16: 60, 6: 60, 1: 57, 18: 55, 2: 52, 17: 51, 32: 49, 26: 43, 14: 41, 25: 37, 8: 26, 10: 23})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "print(collections.Counter(assigned_clusters))\n",
    "# give length of each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Performance and its Standard Deviation of Each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfm = pd.read_csv (r'performance.csv')  \n",
    "pfm = pfm.loc[pfm[\"year\"] == 2019]\n",
    "#backup new csv\n",
    "#pfm.to_csv('pfm19.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>performance</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1750</td>\n",
       "      <td>0.011929</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6201</td>\n",
       "      <td>0.028102</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3197</td>\n",
       "      <td>0.043332</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1230869</td>\n",
       "      <td>0.319006</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>764622</td>\n",
       "      <td>0.029131</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CIK  performance  year\n",
       "0     1750     0.011929  2019\n",
       "1     6201     0.028102  2019\n",
       "2     3197     0.043332  2019\n",
       "3  1230869     0.319006  2019\n",
       "4   764622     0.029131  2019"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need: left join (asg5 csv, pfm19)\n",
    "asg5 = pd.read_csv (r'ASG5.csv') \n",
    "pfm = pd.read_csv (r'pfm19.csv') \n",
    "pfm = pfm.rename(columns={'cik': 'CIK'})\n",
    "pfm[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = asg5.join(pfm.set_index('CIK'), on='CIK')\n",
    "#df.join(other.set_index('key'), on='key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop last column\n",
    "table1 = table1.iloc[: , :-1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to drop all NA on performance later\n",
    "#table1.to_csv('table1.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table1_test = table1[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table1_test.to_csv('table1_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4604"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(assigned_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1['clstr'] = assigned_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3753"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table1 = table1.dropna()\n",
    "len(table1)                 #3753"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>coName</th>\n",
       "      <th>formtype</th>\n",
       "      <th>date</th>\n",
       "      <th>fName</th>\n",
       "      <th>length</th>\n",
       "      <th>full_link</th>\n",
       "      <th>performance</th>\n",
       "      <th>clstr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1345016</td>\n",
       "      <td>YELP INC</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>280603.txt</td>\n",
       "      <td>152323</td>\n",
       "      <td>/blue/acg7849/share/BS/item1/280603.txt</td>\n",
       "      <td>0.038182</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1772177</td>\n",
       "      <td>KURA SUSHI USA, INC.</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td>275323.txt</td>\n",
       "      <td>38468</td>\n",
       "      <td>/blue/acg7849/share/BS/item1/275323.txt</td>\n",
       "      <td>0.019055</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1341766</td>\n",
       "      <td>Celsius Holdings, Inc.</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>280595.txt</td>\n",
       "      <td>132197</td>\n",
       "      <td>/blue/acg7849/share/BS/item1/280595.txt</td>\n",
       "      <td>0.110321</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1679826</td>\n",
       "      <td>Ping Identity Holding Corp.</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>283125.txt</td>\n",
       "      <td>100202</td>\n",
       "      <td>/blue/acg7849/share/BS/item1/283125.txt</td>\n",
       "      <td>-0.001723</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1650445</td>\n",
       "      <td>Quorum Health Corp</td>\n",
       "      <td>10-K</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>282859.txt</td>\n",
       "      <td>146425</td>\n",
       "      <td>/blue/acg7849/share/BS/item1/282859.txt</td>\n",
       "      <td>-0.109879</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CIK                       coName formtype        date       fName  \\\n",
       "0  1345016                     YELP INC     10-K  2019-12-31  280603.txt   \n",
       "1  1772177         KURA SUSHI USA, INC.     10-K  2019-08-31  275323.txt   \n",
       "2  1341766       Celsius Holdings, Inc.     10-K  2019-12-31  280595.txt   \n",
       "3  1679826  Ping Identity Holding Corp.     10-K  2019-12-31  283125.txt   \n",
       "4  1650445           Quorum Health Corp     10-K  2019-12-31  282859.txt   \n",
       "\n",
       "   length                                full_link  performance  clstr  \n",
       "0  152323  /blue/acg7849/share/BS/item1/280603.txt     0.038182     36  \n",
       "1   38468  /blue/acg7849/share/BS/item1/275323.txt     0.019055     21  \n",
       "2  132197  /blue/acg7849/share/BS/item1/280595.txt     0.110321     43  \n",
       "3  100202  /blue/acg7849/share/BS/item1/283125.txt    -0.001723     24  \n",
       "4  146425  /blue/acg7849/share/BS/item1/282859.txt    -0.109879      1  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table1[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1.to_csv('table1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate sd(perf) by clstr\n",
    "#table1_test.sql('SELECT performance,clstr, sum(performance) GROUP BY clstr')\n",
    "table1_sd = table1.groupby('clstr').std()\n",
    "table1_sd = table1_sd ['performance']# we get sd of performance, ranked by cluster\n",
    "#print(table1_sd)\n",
    "table1_sd.to_csv('table1_sd.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract 4-digit SIC Industry Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: w3lib in ./.local/lib/python3.8/site-packages (1.21.0)\n",
      "Requirement already satisfied: first in ./.local/lib/python3.8/site-packages (2.0.2)\n",
      "Requirement already satisfied: six>=1.4.1 in /apps/jupyterhub/1.1.0/lib/python3.8/site-packages (from w3lib) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "# Install Packages\n",
    "\n",
    "!pip install w3lib first\n",
    "\n",
    "import html, re\n",
    "# need to do: pip install w3lib first\n",
    "from w3lib.html import replace_entities\n",
    "\n",
    "import csv\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import glob as glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydirectory = '/blue/acg7849/share/10Ks'\n",
    "file_list = glob.glob(mydirectory + '/*.txt')\n",
    "#print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIC_list = []\n",
    "fileName_list = []\n",
    "\n",
    "for file_path in file_list:\n",
    "    # read the file\n",
    "    with open(file_path) as f:\n",
    "        filing = f.read()\n",
    "\n",
    "    SICregex = 'CLASSIFICATION:.*?\\[(\\d{4})'\n",
    "    SIC = re.findall(SICregex, filing) \n",
    "    SIC_list.append(SIC);\n",
    "        \n",
    "    #need to input FileName\n",
    "    shortName = file_path[file_path.rfind('/')+1:]  \n",
    "    fileName_list.append(shortName);\n",
    "      \n",
    "    DF = pd.DataFrame(list(zip(fileName_list, SIC_list)), columns = ['FileName','Standard Industrial Classification'])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF\n",
    "DF.to_csv('SIC.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use T-test to Evaluate Difference between 2 Sets of 50 Standard Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from numpy import mean\n",
    "from scipy.stats import t\n",
    " \n",
    "# function for calculating the t-test for two dependent samples\n",
    "\n",
    "# change 'data2' with our updated dataframe that includes SIC tag\n",
    "\n",
    "def dependent_ttest(table1_sd, data2, alpha):\n",
    "    # calculate means\n",
    "    mean1, mean2 = mean(table1_sd), mean(data2)\n",
    "    # number of paired samples\n",
    "    n = len(table1_sd)\n",
    "    # sum squared difference between observations\n",
    "    d1 = sum([(table1_sd[i]-data2[i])**2 for i in range(n)])\n",
    "    # sum difference between observations\n",
    "    d2 = sum([table1_sd[i]-data2[i] for i in range(n)])\n",
    "    # standard deviation of the difference between means\n",
    "    sd = sqrt((d1 - (d2**2 / n)) / (n - 1))\n",
    "    # standard error of the difference between the means\n",
    "    sed = sd / sqrt(n)\n",
    "    # calculate the t statistic\n",
    "    t_stat = (mean1 - mean2) / sed\n",
    "    # degrees of freedom\n",
    "    df = n - 1\n",
    "    # calculate the critical value\n",
    "    cv = t.ppf(1.0 - alpha, df)\n",
    "    # calculate the p-value\n",
    "    p = (1.0 - t.cdf(abs(t_stat), df)) * 2.0\n",
    "    # return everything\n",
    "    return t_stat, df, cv, p\n",
    " \n",
    "# seed the random number generator\n",
    "seed(1)\n",
    "# generate two independent samples (pretend they are dependent)\n",
    "table1_sd = 5 * randn(100) + 50\n",
    "data2 = 5 * randn(100) + 51\n",
    "# calculate the t test\n",
    "alpha = 0.05\n",
    "t_stat, df, cv, p = dependent_ttest(table1_sd, data2, alpha)\n",
    "print('t=%.3f, df=%d, cv=%.3f, p=%.3f' % (t_stat, df, cv, p))\n",
    "# interpret via critical value\n",
    "if abs(t_stat) <= cv:\n",
    "    print('Accept null hypothesis that the means are equal.')\n",
    "else:\n",
    "    print('Reject the null hypothesis that the means are equal.')\n",
    "# interpret via p-value\n",
    "if p > alpha:\n",
    "    print('Accept null hypothesis that the means are equal.')\n",
    "else:\n",
    "    print('Reject the null hypothesis that the means are equal.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (basic)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

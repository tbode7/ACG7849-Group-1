{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASG5\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the business sections (/blue/acg7849/share/BS) using Doc2Vec (50 clusters) in two ways:\n",
    "\n",
    "Using a counter as the ‘tag’ (as in 5.1.5)\n",
    "\n",
    "Using a counter as the ‘tag’, and the industry code as an additional tag (yield TaggedDocument(words=file_tokens, tags=[i, SIC]) where SIC is a string holding the tag (for example ‘1740’)\n",
    "\n",
    "Extract the 4-digit SIC industry code from the annual report header (STANDARD INDUSTRIAL CLASSIFICATION).\n",
    "\n",
    "Required: Evaluate whether adding the industry code as an additional tag improves the clustering. Use the standard deviation of profitability as a way to evaluate this. (Firms that are more similar, should have similar performance. Therefore, a better clustering would result in lower standard deviations for each cluster, relative to a worse clustering).\n",
    "\n",
    "Do this for the filings for the year 2019 only. Calculate the standard deviation of performance for each cluster (use the year of CONFORMED END OF PERIOD, which are the first 4 digits of ‘date’ in summary.text).\n",
    "\n",
    "For 50 clusters that means you will have 2 standard deviations for each cluster (one for each approach, with the extra SIC tag vs not adding the extra SIC tag). Use a t-test to test for a difference between the two sets of 50 standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/jupyterhub/1.1.0/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# all imports\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import html, re\n",
    "from w3lib.html import replace_entities\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from scipy.cluster import  hierarchy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "stopWords = set(stopwords.words('english') )\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# table for 2019 filings with >=1000 characters, with links accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv (r'ASG4.csv')  # did in the last assignment\n",
    "mydirectory = '/blue/acg7849/hli1/item1'\n",
    "newdirectory = '/blue/acg7849/share/BS/item1'\n",
    "\n",
    "df[\"full_link\"]= newdirectory +'/'+ df[\"fName\"]\n",
    "#print (sumdf[0:13]) \n",
    "listoflink = list(df[\"full_link\"]) # in case we need a list\n",
    "listofmylink = list(mydirectory +'/'+ df[\"fName\"])\n",
    "\n",
    "#print (listofmylink[0:5])\n",
    "#print (df)\n",
    "\n",
    "df.to_csv('ASG5.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv (r'ASG5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[\"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"length\"]= pd.to_numeric(df[\"length\"])\n",
    "# filter out all the things with <1000 length \n",
    "df = df.loc[df[\"length\"] >= 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  Unnamed: 0.1      CIK                 coName formtype  \\\n",
      "0               0             1  1289850      NeuroMetrix, Inc.     10-K   \n",
      "1               1             2  1345016               YELP INC     10-K   \n",
      "2               2             3  1245791      GI DYNAMICS, INC.     10-K   \n",
      "3               3             4  1772177   KURA SUSHI USA, INC.     10-K   \n",
      "4               4             5   832428        E.W. SCRIPPS Co     10-K   \n",
      "...           ...           ...      ...                    ...      ...   \n",
      "20848       20848         20849  1573221      TheRealReal, Inc.     10-K   \n",
      "20849       20849         20850  1655008  HAHA GENERATION CORP.     10-K   \n",
      "20850       20850         20851    54381             KAMAN Corp     10-K   \n",
      "20851       20851         20852  1590976     Malibu Boats, Inc.     10-K   \n",
      "20852       20852         20853  1419041            Tocagen Inc     10-K   \n",
      "\n",
      "             date       fName  length                                full_link  \n",
      "0      20171231.0  267762.txt  132616  /blue/acg7849/share/BS/item1/267762.txt  \n",
      "1      20191231.0  280603.txt  152323  /blue/acg7849/share/BS/item1/280603.txt  \n",
      "2      20171231.0  267435.txt   28236  /blue/acg7849/share/BS/item1/267435.txt  \n",
      "3      20190831.0  275323.txt   38468  /blue/acg7849/share/BS/item1/275323.txt  \n",
      "4      20181231.0  277539.txt  197843  /blue/acg7849/share/BS/item1/277539.txt  \n",
      "...           ...         ...     ...                                      ...  \n",
      "20848  20191231.0  282124.txt   19581  /blue/acg7849/share/BS/item1/282124.txt  \n",
      "20849  20191231.0  282912.txt   10895  /blue/acg7849/share/BS/item1/282912.txt  \n",
      "20850  20171231.0  276068.txt   85198  /blue/acg7849/share/BS/item1/276068.txt  \n",
      "20851  20190630.0  272211.txt  149561  /blue/acg7849/share/BS/item1/272211.txt  \n",
      "20852  20171231.0  269435.txt  166136  /blue/acg7849/share/BS/item1/269435.txt  \n",
      "\n",
      "[14359 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           CIK                 coName formtype        date       fName  \\\n",
      "0      1289850      NeuroMetrix, Inc.     10-K  20171231.0  267762.txt   \n",
      "1      1345016               YELP INC     10-K  20191231.0  280603.txt   \n",
      "2      1245791      GI DYNAMICS, INC.     10-K  20171231.0  267435.txt   \n",
      "3      1772177   KURA SUSHI USA, INC.     10-K  20190831.0  275323.txt   \n",
      "4       832428        E.W. SCRIPPS Co     10-K  20181231.0  277539.txt   \n",
      "...        ...                    ...      ...         ...         ...   \n",
      "20848  1573221      TheRealReal, Inc.     10-K  20191231.0  282124.txt   \n",
      "20849  1655008  HAHA GENERATION CORP.     10-K  20191231.0  282912.txt   \n",
      "20850    54381             KAMAN Corp     10-K  20171231.0  276068.txt   \n",
      "20851  1590976     Malibu Boats, Inc.     10-K  20190630.0  272211.txt   \n",
      "20852  1419041            Tocagen Inc     10-K  20171231.0  269435.txt   \n",
      "\n",
      "       length                                full_link  \n",
      "0      132616  /blue/acg7849/share/BS/item1/267762.txt  \n",
      "1      152323  /blue/acg7849/share/BS/item1/280603.txt  \n",
      "2       28236  /blue/acg7849/share/BS/item1/267435.txt  \n",
      "3       38468  /blue/acg7849/share/BS/item1/275323.txt  \n",
      "4      197843  /blue/acg7849/share/BS/item1/277539.txt  \n",
      "...       ...                                      ...  \n",
      "20848   19581  /blue/acg7849/share/BS/item1/282124.txt  \n",
      "20849   10895  /blue/acg7849/share/BS/item1/282912.txt  \n",
      "20850   85198  /blue/acg7849/share/BS/item1/276068.txt  \n",
      "20851  149561  /blue/acg7849/share/BS/item1/272211.txt  \n",
      "20852  166136  /blue/acg7849/share/BS/item1/269435.txt  \n",
      "\n",
      "[14359 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "df = df.iloc[: , 1:]\n",
    "df = df.iloc[: , 1:]\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved as backup\n",
    "#df.to_csv('ASG5.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the year 2019\n",
    "df[\"date\"]= pd.to_datetime(df[\"date\"], format='%Y%m%d', errors='ignore')\n",
    "#print(df[\"date\"])\n",
    "#print(df)\n",
    "start_date = \"2019-1-1\"\n",
    "end_date = \"2019-12-31\"\n",
    "\n",
    "after_start_date = df[\"date\"] >= start_date\n",
    "before_end_date = df[\"date\"] <= end_date\n",
    "between_two_dates = after_start_date & before_end_date\n",
    "df = df.loc[between_two_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           CIK                             coName formtype       date  \\\n",
      "1      1345016                           YELP INC     10-K 2019-12-31   \n",
      "3      1772177               KURA SUSHI USA, INC.     10-K 2019-08-31   \n",
      "8      1341766             Celsius Holdings, Inc.     10-K 2019-12-31   \n",
      "9      1679826        Ping Identity Holding Corp.     10-K 2019-12-31   \n",
      "12     1650445                 Quorum Health Corp     10-K 2019-12-31   \n",
      "...        ...                                ...      ...        ...   \n",
      "20843   922224                           PPL Corp     10-K 2019-12-31   \n",
      "20846  1461755  Atlantic Capital Bancshares, Inc.     10-K 2019-12-31   \n",
      "20848  1573221                  TheRealReal, Inc.     10-K 2019-12-31   \n",
      "20849  1655008              HAHA GENERATION CORP.     10-K 2019-12-31   \n",
      "20851  1590976                 Malibu Boats, Inc.     10-K 2019-06-30   \n",
      "\n",
      "            fName  length                                full_link  \n",
      "1      280603.txt  152323  /blue/acg7849/share/BS/item1/280603.txt  \n",
      "3      275323.txt   38468  /blue/acg7849/share/BS/item1/275323.txt  \n",
      "8      280595.txt  132197  /blue/acg7849/share/BS/item1/280595.txt  \n",
      "9      283125.txt  100202  /blue/acg7849/share/BS/item1/283125.txt  \n",
      "12     282859.txt  146425  /blue/acg7849/share/BS/item1/282859.txt  \n",
      "...           ...     ...                                      ...  \n",
      "20843  284298.txt  281462  /blue/acg7849/share/BS/item1/284298.txt  \n",
      "20846  281334.txt   60451  /blue/acg7849/share/BS/item1/281334.txt  \n",
      "20848  282124.txt   19581  /blue/acg7849/share/BS/item1/282124.txt  \n",
      "20849  282912.txt   10895  /blue/acg7849/share/BS/item1/282912.txt  \n",
      "20851  272211.txt  149561  /blue/acg7849/share/BS/item1/272211.txt  \n",
      "\n",
      "[4604 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved as backup\n",
    "#df.to_csv('ASG5.csv', index = False)\n",
    "\n",
    "# so we got: table for 2019 filings with >=1000 characters, with links accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try generator and copy&paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/blue/acg7849/share/BS/item1/280603.txt', '/blue/acg7849/share/BS/item1/275323.txt', '/blue/acg7849/share/BS/item1/280595.txt', '/blue/acg7849/share/BS/item1/283125.txt', '/blue/acg7849/share/BS/item1/282859.txt']\n"
     ]
    }
   ],
   "source": [
    "listoflink = list(df[\"full_link\"]) # in case we need a list\n",
    "print (listoflink[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator function that returns one file at the time (just a string) - csv\n",
    "# note that fit_transform expects one string for each file, so do not tokenize it\n",
    "# this would be different for doc2vec, which expects a taggeddocument element\n",
    "\n",
    "#files = \n",
    "\n",
    "def readBSGen():\n",
    "    for i in listoflink[0:80]:                                                           \n",
    "        with open ( i , encoding='utf-8') as b:\n",
    "            BS = b.read()\n",
    "        yield BS\n",
    "# open the files with listed in csv ASG5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# read documents using generator\n",
    "tfidf = vectorizer.fit_transform( readBSGen(  ) )                # used readBSGen function and files w/yield\n",
    "# dense\n",
    "tfidf = tfidf.todense()                                   # this returns a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.00466174 0.00678589 ... 0.         0.         0.        ]\n",
      " [0.0050993  0.02528593 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.02800195 0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.08022802 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.00563861 0.00978607 0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print (tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#c = np.savetxt('geekfile1.txt', tfidf, delimiter =', ') #1500 files = 3.4G large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4604"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(listoflink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try copy and paste targets - OK to run\n",
    "import shutil\n",
    "\n",
    "for i in range(len(listoflink)):\n",
    "    #shutil.copy(item[0], \"/blue/acg7849/hli1/item1\")\n",
    "    shutil.copy(listoflink[i], r'/blue/acg7849/hli1/item1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (basic)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

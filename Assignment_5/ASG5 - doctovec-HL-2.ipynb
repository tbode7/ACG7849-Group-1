{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASG5\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the business sections (/blue/acg7849/share/BS) using Doc2Vec (50 clusters) in two ways:\n",
    "\n",
    "Using a counter as the ‘tag’ (as in 5.1.5)\n",
    "\n",
    "Using a counter as the ‘tag’, and the industry code as an additional tag (yield TaggedDocument(words=file_tokens, tags=[i, SIC]) where SIC is a string holding the tag (for example ‘1740’)\n",
    "\n",
    "Extract the 4-digit SIC industry code from the annual report header (STANDARD INDUSTRIAL CLASSIFICATION).\n",
    "\n",
    "Required: Evaluate whether adding the industry code as an additional tag improves the clustering. Use the standard deviation of profitability as a way to evaluate this. (Firms that are more similar, should have similar performance. Therefore, a better clustering would result in lower standard deviations for each cluster, relative to a worse clustering).\n",
    "\n",
    "Do this for the filings for the year 2019 only. Calculate the standard deviation of performance for each cluster (use the year of CONFORMED END OF PERIOD, which are the first 4 digits of ‘date’ in summary.text).\n",
    "\n",
    "For 50 clusters that means you will have 2 standard deviations for each cluster (one for each approach, with the extra SIC tag vs not adding the extra SIC tag). Use a t-test to test for a difference between the two sets of 50 standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/jupyterhub/1.1.0/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# all imports\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import html, re\n",
    "from w3lib.html import replace_entities\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from scipy.cluster import  hierarchy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "stopWords = set(stopwords.words('english') )\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# already made this - copy + paste to personal directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#try copy and past targets\n",
    "#think this is a pretty bold way avoiding messing up with the class\n",
    "\n",
    "import shutil\n",
    "for i in range(len(listoflink)):\n",
    "    shutil.copy(listoflink[i], r'/blue/acg7849/hli1/item1')\n",
    "    #shutil.copy(listoflink[i], r'/blue/acg7849/tbode/whateverfile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# codes from 515 - start with new directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some punctuation to string.punctuation\n",
    "punc = string.punctuation + '“”'\n",
    "\n",
    "# documents get tagged by an index (number), while filenames have different numbers\n",
    "# keep track of this\n",
    "fileIdToIndex = {} # given a fileId -> tag\n",
    "indexToFileId=[] # given a tag -> fileId\n",
    "\n",
    "class MyFiles(object):\n",
    "    def __init__(self, dirname, tokens_only = False):\n",
    "        self.dirname = dirname\n",
    "        self.tokens_only = tokens_only\n",
    " \n",
    "    def __iter__(self):\n",
    "        #for i, fname in enumerate(files[0:200]):\n",
    "         \n",
    "        for i, fname in enumerate(os.listdir(self.dirname)[0:200]):\n",
    "        # enumerate = return a list of tuples, iterate from start to end, [0:200] must be there\n",
    "        # os.listdir = return index of a directory, input = directory address\n",
    "        # this part enumerates the first 200 units in the index under dirname\n",
    "        \n",
    "            with open( os.path.join(self.dirname, fname), encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            # filter\n",
    "                #content = [f for f in content if int(f[\"length\"])> 1000 and (f[\"date\"][0:4]) == '2019']\n",
    "                #content = [f for f in content if len(f)> 1000]\n",
    "            # grab id from filename\n",
    "            myCounter = int ( re.findall(r'(\\d*)\\.txt', fname)[0] )\n",
    "            # update \n",
    "            fileIdToIndex [ myCounter] = i\n",
    "            indexToFileId.append( myCounter)\n",
    "            #print('fname', fname, 'tag', myCounter)\n",
    "            file_tokens = [x for x in word_tokenize(content) if x.isalpha() and x.lower() not in stopWords and x not in string.punctuation]\n",
    "            \n",
    "            if self.tokens_only == True:\n",
    "                yield file_tokens\n",
    "            else:\n",
    "                yield TaggedDocument(words=file_tokens, tags=[i] )                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hipergator\n",
    "ffiles = MyFiles(r'/blue/acg7849/hli1/item1') # this one expected str, bytes or os.PathLike object\n",
    "#ffiles = MyFiles(r'/blue/acg7849/share/BS/item1/') # a memory-friendly iterator\n",
    "# dirname = '/blue/acg7849/share/BS/item1/'?\n",
    "\n",
    "# create a model, build vocabulary\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=2, epochs=10)\n",
    "model.build_vocab(ffiles) \n",
    "# train it\n",
    "model.train(ffiles, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hipergator\n",
    "def tokenizeFile(file_id):\n",
    "    with open( r'/blue/acg7849/hli1/item1/'+str(file_id)+'.txt', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    return ([x for x in word_tokenize(content) if x.isalpha() and x.lower() not in stopWords and x not in string.punctuation] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.811593  ,  0.2716504 , -1.6636219 , -2.0849779 ,  3.5197663 ,\n",
       "       -2.5536299 , -2.6030385 , -1.3686801 , -3.014339  ,  0.21578524,\n",
       "       -0.23528711,  2.5992036 , -1.4219581 ,  0.98291737,  0.32460353,\n",
       "        0.9410857 ,  0.92102945,  2.349767  , -1.6231432 , -5.011881  ,\n",
       "       -0.65269136,  0.48169163, -0.05643673,  0.3300975 , -0.41524637,\n",
       "       -0.29054004, -2.1311615 ,  0.36154285, -1.6587504 ,  0.9253874 ,\n",
       "        0.2919745 ,  1.3644869 , -1.2212223 , -1.4661137 ,  0.7169397 ,\n",
       "       -1.2937855 , -0.31634545, -3.0772135 ,  1.0376463 ,  0.16531998,\n",
       "       -0.9418244 , -0.5249591 ,  1.666985  ,  2.3974621 , -0.34267205,\n",
       "       -2.543288  ,  0.40126285,  2.3897433 ,  0.38184747,  0.13161495,\n",
       "       -1.2011691 ,  1.4179391 ,  1.0272518 ,  0.9971536 , -2.0866113 ,\n",
       "       -0.5776119 , -1.9123391 ,  1.6437991 , -0.6308292 ,  1.6901064 ,\n",
       "        3.4923859 ,  0.1999571 ,  2.418181  , -1.1094514 , -1.3805083 ,\n",
       "       -0.8175258 ,  0.98982745,  0.3472884 ,  0.5286452 ,  1.2395036 ,\n",
       "       -2.7854264 ,  0.9542797 , -0.67193294, -1.1917142 ,  1.436595  ,\n",
       "       -0.3667319 , -0.6952395 , -2.4641948 , -0.11002904,  0.6355129 ,\n",
       "        0.8874404 ,  0.12520932,  1.1098694 , -1.2020704 ,  1.59419   ,\n",
       "        3.8328128 , -1.9316268 , -2.3126302 , -0.36431178, -0.9292123 ,\n",
       "       -1.9904907 ,  4.349781  , -1.5573808 , -1.6137192 ,  0.7611109 ,\n",
       "        0.8857136 , -1.5970625 , -0.48650843,  0.4756964 , -0.29619625],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizeFile(277350) # test file = 277350\n",
    "model.infer_vector( t )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 0.6917709708213806),\n",
       " (180, 0.689142107963562),\n",
       " (107, 0.6726787686347961),\n",
       " (26, 0.6559200882911682),\n",
       " (37, 0.611767590045929)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizeFile(277350)\n",
    "inferred_vector = model.infer_vector( t )\n",
    "# dv is short for docvecs\n",
    "# most similar files to 277350 is the 4th one 265065?\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "#sims = model.dv.most_similar([inferred_vector], topn=4)\n",
    "sims[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-0bceb90a8426>:2: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  similar_doc = model.docvecs.most_similar(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(103, 0.8206000328063965),\n",
       " (93, 0.7932485342025757),\n",
       " (149, 0.7799310088157654),\n",
       " (55, 0.7510608434677124),\n",
       " (198, 0.7487143278121948),\n",
       " (107, 0.7478072047233582),\n",
       " (115, 0.7472643852233887),\n",
       " (90, 0.7085216641426086),\n",
       " (37, 0.7024698853492737),\n",
       " (38, 0.6997923254966736)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# letter with filename 1.txt is the first letter, so tag is 0\n",
    "similar_doc = model.docvecs.most_similar(0)\n",
    "similar_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents 200\n",
      "model.docvecs 200\n"
     ]
    }
   ],
   "source": [
    "print('number of documents', model.corpus_count)\n",
    "print('model.docvecs', len(model.dv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8914902 ,  1.0365181 , -0.6279697 , -0.65517074,  1.962731  ,\n",
       "       -0.57669824, -5.149925  , -0.21185449, -3.921839  , -3.2669826 ,\n",
       "        1.4117745 ,  2.70079   , -0.72105616,  0.39709362, -2.2376409 ,\n",
       "        0.27832437,  3.3653367 ,  3.3251123 , -2.647636  , -2.531493  ,\n",
       "       -4.2508774 , -0.09885166, -0.10626055, -1.069596  ,  1.5419154 ,\n",
       "       -3.2789903 , -1.6415857 ,  2.1012747 ,  0.32585385, -1.5470833 ,\n",
       "        3.5130355 ,  1.2856466 , -1.6063392 , -0.99121356, -1.2053097 ,\n",
       "       -1.7892944 ,  0.6532468 , -5.162125  ,  0.45646808, -0.74689907,\n",
       "       -0.08406452, -1.5623255 ,  3.4916182 ,  0.447897  , -1.361903  ,\n",
       "       -4.367096  , -0.6598568 ,  0.4116321 ,  1.7339507 ,  1.7754472 ,\n",
       "        3.207348  ,  1.7356949 ,  0.9911989 , -0.49050486, -2.640037  ,\n",
       "        2.4847753 , -1.0687668 ,  1.1662743 ,  1.2444549 ,  5.057472  ,\n",
       "        4.4588814 , -0.09393429,  5.1969934 ,  0.481649  , -0.6853551 ,\n",
       "       -1.8203465 , -2.1596158 ,  0.6645025 ,  0.46371752, -0.5732975 ,\n",
       "        2.5800345 , -0.5484731 ,  0.50642586, -0.5153707 , -1.8924698 ,\n",
       "        1.4106768 ,  0.90836215,  0.3278758 , -2.6780102 , -1.2162396 ,\n",
       "        3.3187656 , -0.6300073 ,  0.12633769,  1.284925  , -1.2528    ,\n",
       "        1.6027545 , -0.21347494, -1.0619538 ,  0.70830745,  2.1491642 ,\n",
       "       -2.670939  ,  4.698071  , -5.0777364 , -1.7068573 , -0.9524889 ,\n",
       "        1.1871783 ,  1.9079158 , -0.5529036 ,  5.439421  , -1.2397665 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hipergator\n",
    "# reread the files, and get the vector for each file\n",
    "# feed vector into k-means algorithm to make clusters\n",
    "wordLists = MyFiles(r'/blue/acg7849/hli1/item1', tokens_only = True) # a memory-friendly iterator\n",
    "vectors = [ model.infer_vector( w ) for w in wordLists]\n",
    "len(vectors)\n",
    "vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "num_clusters = 10\n",
    "kclusterer = KMeansClusterer(num_clusters, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "assigned_clusters = kclusterer.cluster(vectors, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 31, 4: 28, 1: 24, 9: 21, 0: 21, 8: 21, 2: 19, 7: 13, 3: 13, 5: 9})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "print(collections.Counter(assigned_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# also needs: get industrial codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (basic)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASG5\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the business sections (/blue/acg7849/share/BS) using Doc2Vec (50 clusters) in two ways:\n",
    "\n",
    "Using a counter as the ‘tag’ (as in 5.1.5)\n",
    "\n",
    "Using a counter as the ‘tag’, and the industry code as an additional tag (yield TaggedDocument(words=file_tokens, tags=[i, SIC]) where SIC is a string holding the tag (for example ‘1740’)\n",
    "\n",
    "Extract the 4-digit SIC industry code from the annual report header (STANDARD INDUSTRIAL CLASSIFICATION).\n",
    "\n",
    "Required: Evaluate whether adding the industry code as an additional tag improves the clustering. Use the standard deviation of profitability as a way to evaluate this. (Firms that are more similar, should have similar performance. Therefore, a better clustering would result in lower standard deviations for each cluster, relative to a worse clustering).\n",
    "\n",
    "Do this for the filings for the year 2019 only. Calculate the standard deviation of performance for each cluster (use the year of CONFORMED END OF PERIOD, which are the first 4 digits of ‘date’ in summary.text).\n",
    "\n",
    "For 50 clusters that means you will have 2 standard deviations for each cluster (one for each approach, with the extra SIC tag vs not adding the extra SIC tag). Use a t-test to test for a difference between the two sets of 50 standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/jupyterhub/1.1.0/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# all imports\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import html, re\n",
    "from w3lib.html import replace_entities\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from scipy.cluster import  hierarchy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "stopWords = set(stopwords.words('english') )\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# alternative way for files - piazza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4604"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/blue/acg7849/share/BS/summary.txt') as f:\n",
    "    files = [{k: v for k, v in row.items()}\n",
    "        for row in csv.DictReader(f, skipinitialspace=True, delimiter=\"|\")]\n",
    "\n",
    "# files with length > 1000\n",
    "files = [f for f in files if int(f[\"length\"])> 1000 and (f[\"date\"][0:4]) == '2019']\n",
    "#print (files[0:10]) # a list of dict\n",
    "len(files) # =14359 = 4604\n",
    "#type(files) # list\n",
    "\n",
    "# files with year 2019\n",
    "#files = [f for f in files if (f[\"date\"])== \"20191231\"]  # only date = 1231"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### important variable: files - all files we needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator function that returns one file at the time (just a string) - csv\n",
    "# note that fit_transform expects one string for each file, so do not tokenize it\n",
    "# this would be different for doc2vec, which expects a taggeddocument element\n",
    "\n",
    "def readBSGen():\n",
    "    for f in files[0:80]: # first 500 \n",
    "        with open ( '/blue/acg7849/share/BS/item1/{}'.format(f['filename']) , encoding='utf-8') as b:\n",
    "            BS = b.read()\n",
    "        yield BS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# read documents using generator\n",
    "tfidf = vectorizer.fit_transform( readBSGen(  ) )                # used readBSGen function and files w/yield\n",
    "# dense\n",
    "tfidf = tfidf.todense()                                   # this returns a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.00466174 0.00678589 ... 0.         0.         0.        ]\n",
      " [0.0050993  0.02528593 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.02800195 0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.08022802 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.00563861 0.00978607 0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print (tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#c = np.savetxt('geekfile1.txt', tfidf, delimiter =', ') #1500 files = 3.4G large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# codes from 515"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some punctuation to string.punctuation\n",
    "punc = string.punctuation + '“”'\n",
    "\n",
    "# documents get tagged by an index (number), while filenames have different numbers\n",
    "# keep track of this\n",
    "fileIdToIndex = {} # given a fileId -> tag\n",
    "indexToFileId=[] # given a tag -> fileId\n",
    "\n",
    "class MyFiles(object):\n",
    "    def __init__(self, dirname, tokens_only = False):\n",
    "        self.dirname = dirname\n",
    "        self.tokens_only = tokens_only\n",
    " \n",
    "    def __iter__(self):\n",
    "        for i, fname in enumerate(os.listdir(self.dirname)[0:200]):\n",
    "        #for fname in os.listdir(self.dirname):\n",
    "        # enumerate = return a list of tuples, iterate from start to end, [0:200] must be there\n",
    "        # os.listdir = return index of a directory, input = directory address\n",
    "        # this part enumerates the first 200 units in the index under dirname\n",
    "        \n",
    "            with open( os.path.join(self.dirname, fname), encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            # filter\n",
    "                #content = [f for f in content if int(f[\"length\"])> 1000 and (f[\"date\"][0:4]) == '2019']\n",
    "            \n",
    "            # grab id from filename\n",
    "            myCounter = int (  re.findall(r'(\\d*)\\.txt', fname)[0] )\n",
    "            # update \n",
    "            fileIdToIndex [ myCounter] = i\n",
    "            indexToFileId.append( myCounter)\n",
    "            #print('fname', fname, 'tag', myCounter)\n",
    "            file_tokens = [x for x in word_tokenize(content) if x.isalpha() and x.lower() not in stopWords and x not in string.punctuation]\n",
    "            \n",
    "            if self.tokens_only == True:\n",
    "                yield file_tokens\n",
    "            else:\n",
    "                yield TaggedDocument(words=file_tokens, tags=[i] )                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hipergator\n",
    "ffiles = MyFiles(r'/blue/acg7849/share/BS/item1/') # a memory-friendly iterator\n",
    "# dirname = '/blue/acg7849/share/BS/item1/'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyFiles object at 0x2af50fc50790>\n"
     ]
    }
   ],
   "source": [
    "print(ffiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model, build vocabulary\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=2, epochs=10)\n",
    "model.build_vocab(ffiles)\n",
    "# train it\n",
    "model.train(ffiles, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hipergator\n",
    "def tokenizeFile(file_id):\n",
    "    with open( r'/blue/acg7849/share/BS/item1/'+str(file_id)+'.txt', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    return ([x for x in word_tokenize(content) if x.isalpha() and x.lower() not in stopWords and x not in string.punctuation] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.9205102324485779),\n",
       " (36, 0.6702579855918884),\n",
       " (165, 0.6661437749862671),\n",
       " (170, 0.6392346024513245),\n",
       " (19, 0.6283066272735596),\n",
       " (174, 0.6180595755577087),\n",
       " (147, 0.6105557084083557),\n",
       " (2, 0.5762907862663269),\n",
       " (194, 0.5749393105506897),\n",
       " (131, 0.567370593547821),\n",
       " (196, 0.5535136461257935),\n",
       " (195, 0.5491869449615479),\n",
       " (141, 0.534377932548523),\n",
       " (83, 0.5232897400856018),\n",
       " (76, 0.5225411653518677),\n",
       " (122, 0.49498140811920166),\n",
       " (23, 0.4948655664920807),\n",
       " (50, 0.46032992005348206),\n",
       " (178, 0.4539501965045929),\n",
       " (54, 0.44672587513923645),\n",
       " (197, 0.4432719647884369),\n",
       " (69, 0.4347832500934601),\n",
       " (55, 0.4179377555847168),\n",
       " (192, 0.4177309274673462),\n",
       " (123, 0.41336366534233093),\n",
       " (49, 0.3947412967681885),\n",
       " (129, 0.3885799050331116),\n",
       " (119, 0.3856053352355957),\n",
       " (15, 0.37767598032951355),\n",
       " (11, 0.37748435139656067),\n",
       " (91, 0.3748438358306885),\n",
       " (96, 0.3704301416873932),\n",
       " (45, 0.3697308599948883),\n",
       " (56, 0.36824342608451843),\n",
       " (71, 0.3641180396080017),\n",
       " (185, 0.3579240143299103),\n",
       " (162, 0.356899619102478),\n",
       " (87, 0.3548429310321808),\n",
       " (33, 0.3533955216407776),\n",
       " (64, 0.3530257046222687),\n",
       " (145, 0.3516557216644287),\n",
       " (133, 0.3498724102973938),\n",
       " (125, 0.3495851755142212),\n",
       " (61, 0.3395470678806305),\n",
       " (98, 0.3363591432571411),\n",
       " (169, 0.33553367853164673),\n",
       " (155, 0.33455032110214233),\n",
       " (118, 0.33381757140159607),\n",
       " (172, 0.3226286768913269),\n",
       " (72, 0.31902170181274414),\n",
       " (40, 0.3179830014705658),\n",
       " (75, 0.3145313858985901),\n",
       " (78, 0.31436580419540405),\n",
       " (30, 0.3129347562789917),\n",
       " (92, 0.3126472532749176),\n",
       " (46, 0.3095022737979889),\n",
       " (161, 0.30876147747039795),\n",
       " (77, 0.30697131156921387),\n",
       " (16, 0.30526813864707947),\n",
       " (187, 0.30482861399650574),\n",
       " (157, 0.29894205927848816),\n",
       " (102, 0.29844558238983154),\n",
       " (74, 0.29612499475479126),\n",
       " (150, 0.293461412191391),\n",
       " (95, 0.2933107316493988),\n",
       " (26, 0.29045507311820984),\n",
       " (10, 0.2878522276878357),\n",
       " (13, 0.28687357902526855),\n",
       " (48, 0.28508588671684265),\n",
       " (66, 0.2795572578907013),\n",
       " (121, 0.2769246995449066),\n",
       " (149, 0.2767297923564911),\n",
       " (173, 0.2762857675552368),\n",
       " (65, 0.2762792408466339),\n",
       " (111, 0.2751731872558594),\n",
       " (179, 0.2731532156467438),\n",
       " (32, 0.27267491817474365),\n",
       " (182, 0.2663169503211975),\n",
       " (27, 0.26264438033103943),\n",
       " (120, 0.26067352294921875),\n",
       " (124, 0.2606586217880249),\n",
       " (156, 0.25798261165618896),\n",
       " (106, 0.2569335401058197),\n",
       " (105, 0.25252002477645874),\n",
       " (127, 0.2519368529319763),\n",
       " (199, 0.2501542866230011),\n",
       " (41, 0.24416950345039368),\n",
       " (8, 0.2427942454814911),\n",
       " (88, 0.23923487961292267),\n",
       " (58, 0.23710700869560242),\n",
       " (142, 0.23616203665733337),\n",
       " (86, 0.23481662571430206),\n",
       " (190, 0.23407775163650513),\n",
       " (101, 0.23382946848869324),\n",
       " (191, 0.2327609807252884),\n",
       " (29, 0.22694554924964905),\n",
       " (153, 0.22383049130439758),\n",
       " (39, 0.22371810674667358),\n",
       " (159, 0.22192049026489258),\n",
       " (128, 0.22162359952926636),\n",
       " (112, 0.21823237836360931),\n",
       " (113, 0.21234044432640076),\n",
       " (115, 0.21183240413665771),\n",
       " (9, 0.2103792428970337),\n",
       " (193, 0.20879113674163818),\n",
       " (144, 0.19739395380020142),\n",
       " (28, 0.19686637818813324),\n",
       " (93, 0.19449159502983093),\n",
       " (31, 0.19402644038200378),\n",
       " (7, 0.19271007180213928),\n",
       " (117, 0.19052182137966156),\n",
       " (110, 0.18649707734584808),\n",
       " (126, 0.18637092411518097),\n",
       " (81, 0.18553800880908966),\n",
       " (85, 0.18485519289970398),\n",
       " (198, 0.1844709813594818),\n",
       " (166, 0.1836317926645279),\n",
       " (90, 0.1758878231048584),\n",
       " (80, 0.17588457465171814),\n",
       " (163, 0.17211969196796417),\n",
       " (104, 0.16887441277503967),\n",
       " (151, 0.16328375041484833),\n",
       " (89, 0.15349240601062775),\n",
       " (107, 0.15183621644973755),\n",
       " (186, 0.15064144134521484),\n",
       " (6, 0.1505790501832962),\n",
       " (100, 0.15042023360729218),\n",
       " (38, 0.14687004685401917),\n",
       " (99, 0.14667318761348724),\n",
       " (22, 0.14422358572483063),\n",
       " (114, 0.1345188319683075),\n",
       " (52, 0.13226647675037384),\n",
       " (108, 0.1275971680879593),\n",
       " (57, 0.1250426173210144),\n",
       " (60, 0.12411237508058548),\n",
       " (1, 0.12124137580394745),\n",
       " (116, 0.12033209949731827),\n",
       " (130, 0.11932328343391418),\n",
       " (188, 0.11880820244550705),\n",
       " (139, 0.11489160358905792),\n",
       " (59, 0.11045566946268082),\n",
       " (68, 0.10990238189697266),\n",
       " (137, 0.10947875678539276),\n",
       " (62, 0.1046520546078682),\n",
       " (35, 0.10264681279659271),\n",
       " (148, 0.09461387246847153),\n",
       " (160, 0.09134488552808762),\n",
       " (4, 0.0899336189031601),\n",
       " (143, 0.08898032456636429),\n",
       " (177, 0.08879436552524567),\n",
       " (3, 0.08828486502170563),\n",
       " (181, 0.08354326337575912),\n",
       " (158, 0.07935700565576553),\n",
       " (171, 0.07609278708696365),\n",
       " (21, 0.07125473022460938),\n",
       " (18, 0.07076402008533478),\n",
       " (140, 0.0590364933013916),\n",
       " (164, 0.05892368033528328),\n",
       " (84, 0.05816268175840378),\n",
       " (138, 0.057453908026218414),\n",
       " (176, 0.05296216532588005),\n",
       " (63, 0.051419150084257126),\n",
       " (47, 0.050391968339681625),\n",
       " (79, 0.04732614755630493),\n",
       " (67, 0.0449528805911541),\n",
       " (43, 0.04415511339902878),\n",
       " (103, 0.04404779523611069),\n",
       " (135, 0.04117758944630623),\n",
       " (134, 0.03880113735795021),\n",
       " (14, 0.03784041479229927),\n",
       " (136, 0.03674850985407829),\n",
       " (51, 0.030802719295024872),\n",
       " (175, 0.024220990017056465),\n",
       " (94, 0.021557653322815895),\n",
       " (132, 0.019630124792456627),\n",
       " (152, 0.017226172611117363),\n",
       " (109, 0.01640014350414276),\n",
       " (44, 0.012050604447722435),\n",
       " (20, 0.011107034981250763),\n",
       " (168, 0.00799591839313507),\n",
       " (17, 0.0027267595287412405),\n",
       " (154, -0.01185804046690464),\n",
       " (73, -0.019481636583805084),\n",
       " (24, -0.02443058416247368),\n",
       " (70, -0.030068431049585342),\n",
       " (167, -0.03255649656057358),\n",
       " (82, -0.03507821634411812),\n",
       " (189, -0.03568527474999428),\n",
       " (183, -0.03580741956830025),\n",
       " (25, -0.03647420555353165),\n",
       " (184, -0.04034525528550148),\n",
       " (146, -0.04882240295410156),\n",
       " (97, -0.0507967546582222),\n",
       " (53, -0.053884606808423996),\n",
       " (12, -0.06826654821634293),\n",
       " (5, -0.09221166372299194),\n",
       " (180, -0.0945187658071518),\n",
       " (42, -0.10515966266393661),\n",
       " (37, -0.12356032431125641),\n",
       " (34, -0.14352506399154663)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizeFile(267762)\n",
    "inferred_vector = model.infer_vector( t )\n",
    "# dv is short for docvecs\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "#sims = model.dv.most_similar([inferred_vector], topn=4)\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-0bceb90a8426>:2: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  similar_doc = model.docvecs.most_similar(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(36, 0.8087785243988037),\n",
       " (165, 0.7963026762008667),\n",
       " (19, 0.7845235466957092),\n",
       " (170, 0.7723568677902222),\n",
       " (147, 0.7609975337982178),\n",
       " (174, 0.7606906890869141),\n",
       " (2, 0.7382687330245972),\n",
       " (196, 0.7235947251319885),\n",
       " (131, 0.7055116295814514),\n",
       " (194, 0.6995262503623962)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# letter with filename 1.txt is the first letter, so tag is 0\n",
    "similar_doc = model.docvecs.most_similar(0)\n",
    "similar_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents 200\n",
      "model.docvecs 200\n"
     ]
    }
   ],
   "source": [
    "print('number of documents', model.corpus_count)\n",
    "print('model.docvecs', len(model.dv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.27697149e-01, -5.57108355e+00, -8.76148939e-01, -3.13555098e+00,\n",
       "        5.74351406e+00, -3.09132266e+00,  3.92001420e-01, -4.20823765e+00,\n",
       "       -1.64134932e+00, -1.01604569e+00,  1.00201261e+00,  6.66401958e+00,\n",
       "       -1.60218263e+00, -1.07662237e+00, -4.37635899e+00,  1.36437035e+00,\n",
       "        3.61313486e+00, -3.64824319e+00,  2.21086666e-01, -4.05213308e+00,\n",
       "       -3.29910254e+00,  2.12190766e-03,  4.83307242e-01,  9.45488870e-01,\n",
       "        4.69715929e+00,  1.47742617e+00,  4.44642973e+00,  7.16185663e-03,\n",
       "       -1.90026569e+00, -3.62248921e+00,  2.56235147e+00, -6.02982640e-01,\n",
       "       -1.46367574e+00,  6.50530338e-01, -3.38752127e+00,  1.73800576e+00,\n",
       "        1.15068877e+00, -7.62334883e-01, -1.39826822e+00,  2.98995185e+00,\n",
       "       -2.31975842e+00,  1.67648923e+00, -8.86507690e-01, -1.84130120e+00,\n",
       "       -1.80053151e+00,  8.79312381e-02, -1.10911393e+00, -1.52270377e+00,\n",
       "        2.78991723e+00, -2.91873789e+00,  2.14391065e+00, -1.94369113e+00,\n",
       "       -4.65059996e+00,  2.15206528e+00,  3.62731934e+00,  3.99666858e+00,\n",
       "        1.12156436e-01,  7.93166459e-01,  3.30280614e+00,  5.20119810e+00,\n",
       "        1.75592864e+00,  3.25306153e+00, -4.39528465e-01, -2.69794059e+00,\n",
       "        2.88121790e-01,  1.40980625e+00, -4.65689451e-01,  2.82085109e+00,\n",
       "        1.66099691e+00,  2.63892198e+00, -2.03060412e+00,  2.70667529e+00,\n",
       "       -1.42069876e+00, -4.68344975e+00, -3.25894713e+00,  1.23840880e+00,\n",
       "       -8.19998384e-01, -2.48688459e+00, -3.05637383e+00, -1.30820453e-01,\n",
       "       -4.99989949e-02,  1.02920428e-01, -1.29518926e+00,  4.46799427e-01,\n",
       "        5.71069336e+00, -9.76035148e-02,  3.22982311e+00,  6.87195420e-01,\n",
       "        9.35561895e-01, -5.34672797e-01,  7.18395054e-01,  3.12744832e+00,\n",
       "       -4.13860655e+00, -2.63014245e+00, -8.68267000e-01, -2.30605340e+00,\n",
       "       -2.17636299e+00, -1.62203759e-01, -3.44168842e-01,  8.29813853e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hipergator\n",
    "# reread the files, and get the vector for each file\n",
    "# feed vector into k-means algorithm to make clusters\n",
    "wordLists = MyFiles(r'/blue/acg7849/share/BS/item1/', tokens_only = True) # a memory-friendly iterator\n",
    "vectors = [ model.infer_vector( w ) for w in wordLists]\n",
    "len(vectors)\n",
    "vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "num_clusters = 10\n",
    "kclusterer = KMeansClusterer(num_clusters, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "assigned_clusters = kclusterer.cluster(vectors, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({8: 55, 7: 29, 5: 23, 6: 20, 2: 18, 4: 17, 3: 11, 1: 11, 0: 9, 9: 7})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "print(collections.Counter(assigned_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# asdsada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.6 ms, sys: 29 µs, total: 39.6 ms\n",
      "Wall time: 39.8 ms\n",
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clustering\n",
    "threshold = 0.5\n",
    "# Z is the cosine distance matrix\n",
    "%time Z = hierarchy.linkage(tfidf,\"average\", metric=\"cosine\")                              #%time=  time used for this step\n",
    "%time len(Z)                                                                                # match B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 329 µs, sys: 0 ns, total: 329 µs\n",
      "Wall time: 333 µs\n",
      "[35  3 30 44  5 56 58 52 32 19 22 49 42 71 65 58 53 62 63 66 17  8 10 60\n",
      " 59 46 45 47 33 43 38 34 24 61 40  1 31 39 54  6 14 50 62 20 63  2 11 37\n",
      " 15 55 41 23 12 57 69 36 62 21 64 19 51 70 10  9 27 16 13 26 18 48  7 67\n",
      " 68 62 28 62  4 25 29 19]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C are the clusters assigned\n",
    "%time C = hierarchy.fcluster(Z, threshold, criterion=\"distance\")\n",
    "print(C)\n",
    "len(C) #                                                                                 # match C\n",
    "#max(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indexToFileId' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-794d583f5dfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclust\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# look up the filename for i (which is the tag), then add it to the list for that cluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mclust\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mindexToFileId\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m]\u001b[0m  \u001b[0;34m)\u001b[0m               \u001b[0;31m#  list index out of range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# first two clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'indexToFileId' is not defined"
     ]
    }
   ],
   "source": [
    "#keep track of fileIds for letters in different clusters\n",
    "ids = [ [] for i in range(100) ]\n",
    "for i, clust in enumerate(C):\n",
    "    # look up the filename for i (which is the tag), then add it to the list for that cluster\n",
    "    ids[ clust - 1 ].append( indexToFileId[ i ]  )               #  list index out of range\n",
    "\n",
    "# first two clusters\n",
    "ids[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, clust in enumerate(ids):\n",
    "    content = \"\"\n",
    "    for file in clust:\n",
    "        with open( str(file) + \".txt\", encoding='utf-8') as f:# No such file or directory: '266806.txt'\n",
    "            content += f.read()\n",
    "    file_tokens = [x.lower() for x in word_tokenize(content) if x.isalpha() and x.lower() not in stopWords and x not in string.punctuation]\n",
    "    # now we can use nltk functions on the text\n",
    "    fdist = FreqDist(file_tokens)\n",
    "    print ('cluster', i+ 1, 'most common words in this letter', fdist.most_common(5) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (basic)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

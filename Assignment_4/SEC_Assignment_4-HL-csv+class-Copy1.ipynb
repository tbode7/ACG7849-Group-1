{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5 inclass\n",
    "#### Group 1: Tara Bode, Hankun Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A: Text-based industry classification\n",
    "Cluster the business sections extracted in assignment 3 into 50 clusters (using the cosine similarity) to group firms with similar business descriptions together.\n",
    "\n",
    "Submit your code and the final output csv file, with CIK, conformed end of period, and the cluster number for each business section file.\n",
    "\n",
    "Note: it is likely you will need to tokenize/word count each business section file before reading the next one. (If you try to read all business sections before doing any further processing you will probably run out of memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import html, re\n",
    "from w3lib.html import replace_entities\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from scipy.cluster import  hierarchy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to simply take the.txt files from HiperGator and use the path (r'/blue/acg7849/share/BS/') \n",
    "\n",
    "so we can grab the length of each file for the csv file.\n",
    "\n",
    "Also, we need to use the for loop to cluster the business section files and mimic the class code from yesterday to grab the top 20 common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv file, with CIK, conformed end of period, and file name (business section file) and the cluster number for each business section file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # recommended - use summary files - got initial csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CIK|coname|formtype|date|filename|length\\n', '0001289850|NeuroMetrix, Inc.|10-K|20171231|267762.txt|132616\\n', '0001345016|YELP INC|10-K|20191231|280603.txt|152323\\n', '0001245791|GI DYNAMICS, INC.|10-K|20171231|267435.txt|28236\\n', '0001772177|KURA SUSHI USA, INC.|10-K|20190831|275323.txt|38468\\n']\n",
      "           CIK                                             coName formtype  \\\n",
      "1   0001289850                                  NeuroMetrix, Inc.     10-K   \n",
      "2   0001345016                                           YELP INC     10-K   \n",
      "3   0001245791                                  GI DYNAMICS, INC.     10-K   \n",
      "4   0001772177                               KURA SUSHI USA, INC.     10-K   \n",
      "5   0000832428                                    E.W. SCRIPPS Co     10-K   \n",
      "6   0001513761                Norwegian Cruise Line Holdings Ltd.     10-K   \n",
      "7   0001678361  World Omni Automobile Lease Securitization Tru...     10-K   \n",
      "8   0001029199                              EURONET WORLDWIDE INC     10-K   \n",
      "9   0001341766                             Celsius Holdings, Inc.     10-K   \n",
      "10  0001679826                        Ping Identity Holding Corp.     10-K   \n",
      "11  0001549631                                   Quarta-Rad, Inc.     10-K   \n",
      "12  0001077771                               UMPQUA HOLDINGS CORP     10-K   \n",
      "13  0001650445                                 Quorum Health Corp     10-K   \n",
      "14  0000049600                           EASTGROUP PROPERTIES INC     10-K   \n",
      "15  0001464300                      ALL MARKETING SOLUTIONS, INC.     10-K   \n",
      "\n",
      "        date       fName    length  \n",
      "1   20171231  267762.txt  132616\\n  \n",
      "2   20191231  280603.txt  152323\\n  \n",
      "3   20171231  267435.txt   28236\\n  \n",
      "4   20190831  275323.txt   38468\\n  \n",
      "5   20181231  277539.txt  197843\\n  \n",
      "6   20171231  270762.txt  157541\\n  \n",
      "7   20171231  274077.txt     502\\n  \n",
      "8   20171231  265478.txt  172636\\n  \n",
      "9   20191231  280595.txt  132197\\n  \n",
      "10  20191231  283125.txt  100202\\n  \n",
      "11  20191231  281916.txt     540\\n  \n",
      "12  20191231  279623.txt       0\\n  \n",
      "13  20191231  282859.txt  146425\\n  \n",
      "14  20191231  284462.txt   20617\\n  \n",
      "15  20171231  270024.txt    5452\\n  \n"
     ]
    }
   ],
   "source": [
    "# matching filenames\n",
    "\n",
    "with open ('/blue/acg7849/share/BS/summary.txt') as a: \n",
    "    summary = a. readlines()\n",
    "print (summary[0:5])\n",
    "\n",
    "sumdf = pd.DataFrame (summary)\n",
    "#sumdf.str.split(pat=\"|\")\n",
    "\n",
    "new = sumdf[0].str.split(\"|\", expand = True)\n",
    "sumdf[\"CIK\"]= new[0]\n",
    "sumdf[\"coName\"]= new[1]\n",
    "sumdf[\"formtype\"]= new[2]\n",
    "sumdf[\"date\"]= new[3]\n",
    "sumdf[\"fName\"]= new[4]\n",
    "sumdf[\"length\"]= new[5]\n",
    "sumdf = sumdf.drop(columns=[0])\n",
    "sumdf = sumdf.drop([0])\n",
    "\n",
    "print (sumdf[0:15])                        \n",
    "#Initialize csv - no clusters\n",
    "#sumdf.to_csv('ASG4.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           CIK                                             coName formtype  \\\n",
      "1   0001289850                                  NeuroMetrix, Inc.     10-K   \n",
      "2   0001345016                                           YELP INC     10-K   \n",
      "3   0001245791                                  GI DYNAMICS, INC.     10-K   \n",
      "4   0001772177                               KURA SUSHI USA, INC.     10-K   \n",
      "5   0000832428                                    E.W. SCRIPPS Co     10-K   \n",
      "6   0001513761                Norwegian Cruise Line Holdings Ltd.     10-K   \n",
      "7   0001678361  World Omni Automobile Lease Securitization Tru...     10-K   \n",
      "8   0001029199                              EURONET WORLDWIDE INC     10-K   \n",
      "9   0001341766                             Celsius Holdings, Inc.     10-K   \n",
      "10  0001679826                        Ping Identity Holding Corp.     10-K   \n",
      "11  0001549631                                   Quarta-Rad, Inc.     10-K   \n",
      "12  0001077771                               UMPQUA HOLDINGS CORP     10-K   \n",
      "13  0001650445                                 Quorum Health Corp     10-K   \n",
      "14  0000049600                           EASTGROUP PROPERTIES INC     10-K   \n",
      "15  0001464300                      ALL MARKETING SOLUTIONS, INC.     10-K   \n",
      "\n",
      "        date       fName  length  \n",
      "1   20171231  267762.txt  132616  \n",
      "2   20191231  280603.txt  152323  \n",
      "3   20171231  267435.txt   28236  \n",
      "4   20190831  275323.txt   38468  \n",
      "5   20181231  277539.txt  197843  \n",
      "6   20171231  270762.txt  157541  \n",
      "7   20171231  274077.txt     502  \n",
      "8   20171231  265478.txt  172636  \n",
      "9   20191231  280595.txt  132197  \n",
      "10  20191231  283125.txt  100202  \n",
      "11  20191231  281916.txt     540  \n",
      "12  20191231  279623.txt       0  \n",
      "13  20191231  282859.txt  146425  \n",
      "14  20191231  284462.txt   20617  \n",
      "15  20171231  270024.txt    5452  \n"
     ]
    }
   ],
   "source": [
    "sumdf[\"length\"]= pd.to_numeric(sumdf[\"length\"])\n",
    "print (sumdf[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           CIK                               coName formtype      date  \\\n",
      "1   0001289850                    NeuroMetrix, Inc.     10-K  20171231   \n",
      "2   0001345016                             YELP INC     10-K  20191231   \n",
      "3   0001245791                    GI DYNAMICS, INC.     10-K  20171231   \n",
      "4   0001772177                 KURA SUSHI USA, INC.     10-K  20190831   \n",
      "5   0000832428                      E.W. SCRIPPS Co     10-K  20181231   \n",
      "6   0001513761  Norwegian Cruise Line Holdings Ltd.     10-K  20171231   \n",
      "8   0001029199                EURONET WORLDWIDE INC     10-K  20171231   \n",
      "9   0001341766               Celsius Holdings, Inc.     10-K  20191231   \n",
      "10  0001679826          Ping Identity Holding Corp.     10-K  20191231   \n",
      "13  0001650445                   Quorum Health Corp     10-K  20191231   \n",
      "14  0000049600             EASTGROUP PROPERTIES INC     10-K  20191231   \n",
      "15  0001464300        ALL MARKETING SOLUTIONS, INC.     10-K  20171231   \n",
      "17  0001174891                       CalEthos, Inc.     10-K  20191231   \n",
      "20  0001128361                     HOPE BANCORP INC     10-K  20171231   \n",
      "21  0001693696      UNITED CAPITAL CONSULTANTS INC.     10-K  20191231   \n",
      "\n",
      "         fName  length  \n",
      "1   267762.txt  132616  \n",
      "2   280603.txt  152323  \n",
      "3   267435.txt   28236  \n",
      "4   275323.txt   38468  \n",
      "5   277539.txt  197843  \n",
      "6   270762.txt  157541  \n",
      "8   265478.txt  172636  \n",
      "9   280595.txt  132197  \n",
      "10  283125.txt  100202  \n",
      "13  282859.txt  146425  \n",
      "14  284462.txt   20617  \n",
      "15  270024.txt    5452  \n",
      "17  280098.txt   88622  \n",
      "20  266806.txt   99437  \n",
      "21  283268.txt   90266  \n"
     ]
    }
   ],
   "source": [
    "# filter out all the things with <1000 length \n",
    "sumdf = sumdf.loc[sumdf[\"length\"] >= 1000]\n",
    "print (sumdf[0:15])     # seems filtering success! \n",
    "\n",
    "sumdf_bk  = sumdf  # make another file as backup\n",
    "sumdf_test  = sumdf[:100]  # test length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# proceed for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdirectory = '/blue/acg7849/share/BS/item1'\n",
    "#file_list1 = glob.glob(newdirectory + '/*.txt')\n",
    "#print(file_list1) # success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           CIK                               coName formtype      date  \\\n",
      "1   0001289850                    NeuroMetrix, Inc.     10-K  20171231   \n",
      "2   0001345016                             YELP INC     10-K  20191231   \n",
      "3   0001245791                    GI DYNAMICS, INC.     10-K  20171231   \n",
      "4   0001772177                 KURA SUSHI USA, INC.     10-K  20190831   \n",
      "5   0000832428                      E.W. SCRIPPS Co     10-K  20181231   \n",
      "6   0001513761  Norwegian Cruise Line Holdings Ltd.     10-K  20171231   \n",
      "8   0001029199                EURONET WORLDWIDE INC     10-K  20171231   \n",
      "9   0001341766               Celsius Holdings, Inc.     10-K  20191231   \n",
      "10  0001679826          Ping Identity Holding Corp.     10-K  20191231   \n",
      "13  0001650445                   Quorum Health Corp     10-K  20191231   \n",
      "14  0000049600             EASTGROUP PROPERTIES INC     10-K  20191231   \n",
      "15  0001464300        ALL MARKETING SOLUTIONS, INC.     10-K  20171231   \n",
      "17  0001174891                       CalEthos, Inc.     10-K  20191231   \n",
      "\n",
      "         fName  length                                full_link  \n",
      "1   267762.txt  132616  /blue/acg7849/share/BS/item1/267762.txt  \n",
      "2   280603.txt  152323  /blue/acg7849/share/BS/item1/280603.txt  \n",
      "3   267435.txt   28236  /blue/acg7849/share/BS/item1/267435.txt  \n",
      "4   275323.txt   38468  /blue/acg7849/share/BS/item1/275323.txt  \n",
      "5   277539.txt  197843  /blue/acg7849/share/BS/item1/277539.txt  \n",
      "6   270762.txt  157541  /blue/acg7849/share/BS/item1/270762.txt  \n",
      "8   265478.txt  172636  /blue/acg7849/share/BS/item1/265478.txt  \n",
      "9   280595.txt  132197  /blue/acg7849/share/BS/item1/280595.txt  \n",
      "10  283125.txt  100202  /blue/acg7849/share/BS/item1/283125.txt  \n",
      "13  282859.txt  146425  /blue/acg7849/share/BS/item1/282859.txt  \n",
      "14  284462.txt   20617  /blue/acg7849/share/BS/item1/284462.txt  \n",
      "15  270024.txt    5452  /blue/acg7849/share/BS/item1/270024.txt  \n",
      "17  280098.txt   88622  /blue/acg7849/share/BS/item1/280098.txt  \n"
     ]
    }
   ],
   "source": [
    "sumdf[\"full_link\"]= newdirectory +'/'+ sumdf[\"fName\"]\n",
    "print (sumdf[0:13]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/blue/acg7849/share/BS/item1/267762.txt', '/blue/acg7849/share/BS/item1/280603.txt', '/blue/acg7849/share/BS/item1/267435.txt', '/blue/acg7849/share/BS/item1/275323.txt', '/blue/acg7849/share/BS/item1/277539.txt']\n"
     ]
    }
   ],
   "source": [
    "listoflink = list(sumdf[\"full_link\"]) # in case we need a list\n",
    "print (listoflink[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# list of files (raw text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiles_raw = [] \n",
    "\n",
    "# process first 10 files\n",
    "for i in listoflink[0:100]:  # will crash if do all at once\n",
    "    with open( i, encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    # append file to allFiles\n",
    "    allFiles_raw.append(content ) # plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(allFiles_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(allFiles_raw))\n",
    "print(type(allFiles_raw[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# piazza advices - avoid HPG crash (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = (\n",
    "\"The sky is blue\",\n",
    "\"The sun is bright\",\n",
    "\"The sun in sky is bright\",\n",
    "\"We can see the shining sun, bright sun\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 43)\t0.003647863062960956\n",
      "  (0, 19987)\t0.0034155099368387036\n",
      "  (0, 20222)\t0.0025281901344859025\n",
      "  (0, 129)\t0.002760543260608156\n",
      "  (0, 13446)\t0.0013133869940178285\n",
      "  (0, 511)\t0.0020027279340754083\n",
      "  (0, 15838)\t0.0032352828504582393\n",
      "  (0, 9345)\t0.003975346401076231\n",
      "  (0, 17090)\t0.003975346401076231\n",
      "  (0, 10452)\t0.003975346401076231\n",
      "  (0, 5906)\t0.0009759934298421682\n",
      "  (0, 17408)\t0.0012556714974309082\n",
      "  (0, 7099)\t0.001640870332133102\n",
      "  (0, 19525)\t0.00229583700836365\n",
      "  (0, 13336)\t0.0016636231970336927\n",
      "  (0, 19350)\t0.002200706796370629\n",
      "  (0, 17735)\t0.001535829894079695\n",
      "  (0, 11732)\t0.001968353670248376\n",
      "  (0, 8864)\t0.0017881265838679118\n",
      "  (0, 17736)\t0.0030182896197770885\n",
      "  (0, 9331)\t0.0012282901196303852\n",
      "  (0, 2045)\t0.002246872120565892\n",
      "  (0, 55)\t0.002157038145850191\n",
      "  (0, 1681)\t0.001535829894079695\n",
      "  (0, 4289)\t0.0020386304354268214\n",
      "  :\t:\n",
      "  (99, 8979)\t0.03335804470549861\n",
      "  (99, 20049)\t0.18853304183802358\n",
      "  (99, 18184)\t0.2570215639617925\n",
      "  (99, 6214)\t0.057482224480335846\n",
      "  (99, 12018)\t0.023629986363512913\n",
      "  (99, 16614)\t0.017429258662283002\n",
      "  (99, 6707)\t0.0070420731813510395\n",
      "  (99, 14909)\t0.007715944995945481\n",
      "  (99, 4354)\t0.010369644708984719\n",
      "  (99, 9921)\t0.006582183480000027\n",
      "  (99, 10251)\t0.12130198074726768\n",
      "  (99, 20244)\t0.06974863892967892\n",
      "  (99, 10637)\t0.02655677870314977\n",
      "  (99, 9012)\t0.016535593368758306\n",
      "  (99, 13710)\t0.27577727112504585\n",
      "  (99, 10291)\t0.037862020486540665\n",
      "  (99, 2230)\t0.009560179434696192\n",
      "  (99, 18796)\t0.21531101582640008\n",
      "  (99, 6813)\t0.005879379355905676\n",
      "  (99, 2732)\t0.521598517213251\n",
      "  (99, 5207)\t0.006248072601449763\n",
      "  (99, 10891)\t0.051553341817588756\n",
      "  (99, 2698)\t0.021439133732252508\n",
      "  (99, 4171)\t0.04804270412269391\n",
      "  (99, 10929)\t0.005946171640206954\n"
     ]
    }
   ],
   "source": [
    "#tfidf = vectorizer.fit_transform(generator) # should be a txt object/tuple/list of txt?\n",
    "tfidf_test = vectorizer.fit_transform(documents)\n",
    "tfidff = vectorizer.fit_transform(allFiles_raw)\n",
    "print (tfidff)\n",
    "#print (allFiles_raw[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-33-1e80a839aa1e>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-1e80a839aa1e>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    yield documents[0]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "def docGen():\n",
    "    yield documents[0]\n",
    "    yield documents[1]\n",
    "    yield documents[2]\n",
    "    yield documents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-35-1e80a839aa1e>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-1e80a839aa1e>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    yield documents[0]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "def docGen():\n",
    "    yield documents[0]\n",
    "    yield documents[1]\n",
    "    yield documents[2]\n",
    "    yield documents[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docGen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-98251aee3348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'size tfidf matrix (#documents, #unique words):'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unique words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docGen' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf = vectorizer.fit_transform(docGen() )\n",
    "print ('size tfidf matrix (#documents, #unique words):', tfidf.shape)\n",
    "\n",
    "print('unique words', vectorizer.get_feature_names() )\n",
    "\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator\n",
    "class MyCommentLetters(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        #for fname in os.listdir(self.dirname)[0:2]:\n",
    "        for fname in os.listdir(self.dirname)[0:200]:\n",
    "            \n",
    "            for line in open(os.path.join(self.dirname, fname), encoding='utf-8'):\n",
    "                # cleaner (but slower)\n",
    "                yield [ x for x in word_tokenize(line) if x.isalpha() and x.lower() not in stopWords and x not in punc ]\n",
    "                # faster (but not so clean)\n",
    "                #yield line.lower().split()\n",
    "                \n",
    "import gensim\n",
    "# min-count is how minimum count of each word, default is 5\n",
    "# workers: #cores (may need cython to be installed for this to have an effect)\n",
    "model = gensim.models.Word2Vec(wordLists, min_count=2, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kmeans - try with raw files list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "\n",
    "file_list1 = listoflink \n",
    "#print(file_list1) # success!\n",
    "\n",
    "# allFiles_raw is list of string/files\n",
    "# this avoids to read in and combine a list of strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in allFiles_raw:\n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'filing1', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 648158 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print ('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hli1/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.4 s, sys: 122 ms, total: 15.5 s\n",
      "Wall time: 15.6 s\n",
      "(100, 1376)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(allFiles_raw) #fit the vectorizer to a iterable list of txt\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.16 s, sys: 0 ns, total: 1.16 s\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 50                            # doc num > num of clst\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "#uncomment the below to save your model \n",
    "#since I've already run my model I am loading from the pickle\n",
    "joblib.dump(km,  'doc_cluster.pkl')\n",
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 9, 12, 11, 9, 48, 49, 1, 0, 20, 10, 37, 15, 14, 8, 12, 35, 2, 22, 1, 4, 4, 7, 0, 17, 30, 5, 41, 28, 12, 10, 46, 8, 21, 16, 40, 17, 14, 44, 20, 20, 3, 33, 18, 20, 5, 24, 15, 2, 46, 47, 14, 13, 0, 25, 27, 34, 32, 18, 3, 34, 0, 24, 17, 14, 29, 46, 40, 24, 4, 6, 11, 30, 49, 13, 38, 20, 19, 45, 4, 44, 4, 31, 28, 43, 21, 42, 36, 0, 23, 17, 4, 39, 9, 8, 3, 23, 23, 26, 19]\n"
     ]
    }
   ],
   "source": [
    "print(clusters) # success up to now, list of clulsters can be appended to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending clusters to df!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           CIK                               coName formtype      date  \\\n",
      "1   0001289850                    NeuroMetrix, Inc.     10-K  20171231   \n",
      "2   0001345016                             YELP INC     10-K  20191231   \n",
      "3   0001245791                    GI DYNAMICS, INC.     10-K  20171231   \n",
      "4   0001772177                 KURA SUSHI USA, INC.     10-K  20190831   \n",
      "5   0000832428                      E.W. SCRIPPS Co     10-K  20181231   \n",
      "6   0001513761  Norwegian Cruise Line Holdings Ltd.     10-K  20171231   \n",
      "8   0001029199                EURONET WORLDWIDE INC     10-K  20171231   \n",
      "9   0001341766               Celsius Holdings, Inc.     10-K  20191231   \n",
      "10  0001679826          Ping Identity Holding Corp.     10-K  20191231   \n",
      "13  0001650445                   Quorum Health Corp     10-K  20191231   \n",
      "\n",
      "         fName  length  cluster  \n",
      "1   267762.txt  132616       12  \n",
      "2   280603.txt  152323        9  \n",
      "3   267435.txt   28236       12  \n",
      "4   275323.txt   38468       11  \n",
      "5   277539.txt  197843        9  \n",
      "6   270762.txt  157541       48  \n",
      "8   265478.txt  172636       49  \n",
      "9   280595.txt  132197        1  \n",
      "10  283125.txt  100202        0  \n",
      "13  282859.txt  146425       20  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-03c3c01d41ba>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sumdf_test[\"cluster\"]= clusters\n"
     ]
    }
   ],
   "source": [
    "# based on the 100-file version\n",
    "\n",
    "sumdf_test[\"cluster\"]= clusters\n",
    "print (sumdf_test[0:10])  # test success\n",
    "sumdf_test.to_csv('ASG4-test.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (basic)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

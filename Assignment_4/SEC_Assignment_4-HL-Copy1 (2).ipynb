{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "#### Group 1: Tara Bode, Hankun Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A: Text-based industry classification\n",
    "Cluster the business sections extracted in assignment 3 into 50 clusters (using the cosine similarity) to group firms with similar business descriptions together.\n",
    "\n",
    "Submit your code and the final output csv file, with CIK, conformed end of period, and the cluster number for each business section file.\n",
    "\n",
    "Note: it is likely you will need to tokenize/word count each business section file before reading the next one. (If you try to read all business sections before doing any further processing you will probably run out of memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import html, re\n",
    "from w3lib.html import replace_entities\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from scipy.cluster import  hierarchy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to simply take the.txt files from HiperGator and use the path (r'/blue/acg7849/share/BS/') \n",
    "\n",
    "so we can grab the length of each file for the csv file.\n",
    "\n",
    "Also, we need to use the for loop to cluster the business section files and mimic the class code from yesterday to grab the top 20 common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv file, with CIK, conformed end of period, and file name (business section file) and the cluster number for each business section file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # recommended - use summary files - got initial csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CIK|coname|formtype|date|filename|length\\n', '0001289850|NeuroMetrix, Inc.|10-K|20171231|267762.txt|132616\\n', '0001345016|YELP INC|10-K|20191231|280603.txt|152323\\n', '0001245791|GI DYNAMICS, INC.|10-K|20171231|267435.txt|28236\\n', '0001772177|KURA SUSHI USA, INC.|10-K|20190831|275323.txt|38468\\n']\n",
      "          CIK                coName formtype      date       fName    length\n",
      "1  0001289850     NeuroMetrix, Inc.     10-K  20171231  267762.txt  132616\\n\n",
      "2  0001345016              YELP INC     10-K  20191231  280603.txt  152323\\n\n",
      "3  0001245791     GI DYNAMICS, INC.     10-K  20171231  267435.txt   28236\\n\n",
      "4  0001772177  KURA SUSHI USA, INC.     10-K  20190831  275323.txt   38468\\n\n",
      "5  0000832428       E.W. SCRIPPS Co     10-K  20181231  277539.txt  197843\\n\n"
     ]
    }
   ],
   "source": [
    "# matching filenames\n",
    "\n",
    "with open ('/blue/acg7849/share/BS/summary.txt') as a: \n",
    "    summary = a. readlines()\n",
    "print (summary[0:5])\n",
    "\n",
    "sumdf = pd.DataFrame (summary)\n",
    "#sumdf.str.split(pat=\"|\")\n",
    "\n",
    "\n",
    "new = sumdf[0].str.split(\"|\", expand = True)\n",
    "sumdf[\"CIK\"]= new[0]\n",
    "sumdf[\"coName\"]= new[1]\n",
    "sumdf[\"formtype\"]= new[2]\n",
    "sumdf[\"date\"]= new[3]\n",
    "sumdf[\"fName\"]= new[4]\n",
    "sumdf[\"length\"]= new[5]\n",
    "sumdf = sumdf.drop(columns=[0])\n",
    "sumdf = sumdf.drop([0])\n",
    "\n",
    "print (sumdf[0:5])                        \n",
    "#Initialize csv - no clusters\n",
    "sumdf.to_csv('ASG4.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new loop for clustering (possibly not good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdirectory = '/blue/acg7849/share/BS/item1'\n",
    "file_list1 = glob.glob(newdirectory + '/*.txt')\n",
    "#print(file_list1) # success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = pd.DataFrame(file_list)\n",
    "obj.to_csv('file_list.csv', index = True) # csv preapared to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badFiles_raw = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in file_list[0:5]:\n",
    "    print (i)\n",
    "    # read the file\n",
    "    with open(i) as f:\n",
    "        filing = f.read()\n",
    "        \n",
    "    # grab the length of the file\n",
    "    print (len(filing))\n",
    "\n",
    "    # Ignore business section files with less than 1000 characters.\n",
    "    if len(filing) < 50000: # test. should be 1000 here\n",
    "        print ('bad')\n",
    "        badFiles_raw.append (i)\n",
    "    # should end this file and return to next one\n",
    "    # No. of cluster should be 0 or NA\n",
    "\n",
    "    \n",
    "    #need to input FileName\n",
    "    shortName = i[i.rfind(\"/\")+1:]\n",
    "    print(shortName)\n",
    "    # match shortnames to cluster No.\n",
    "    \n",
    "    # tokenize\n",
    "    #word_tokenize (filing[0:10]) # bit different from split - include punctuations\n",
    "#print(badFiles_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do K-means (important)\n",
    "## reference: http://brandonrose.org/clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make a list of full texts to do K-means\n",
    "## reference: http://brandonrose.org/clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  /blue/acg7849/share/BS/item1/267762.txt\n",
      "length  1\n",
      "-1\n",
      "big  1\n",
      "i =  /blue/acg7849/share/BS/item1/280603.txt\n",
      "length  1\n",
      "0\n",
      "big  2\n",
      "i =  /blue/acg7849/share/BS/item1/267435.txt\n",
      "length  489\n",
      "1\n",
      "long 1\n",
      "big  4\n",
      "i =  /blue/acg7849/share/BS/item1/275323.txt\n",
      "length  176\n",
      "2\n",
      "long 2\n",
      "big  7\n",
      "i =  /blue/acg7849/share/BS/item1/277539.txt\n",
      "length  1\n",
      "3\n",
      "big  8\n",
      "last 1\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "\n",
    "newdirectory = '/blue/acg7849/share/BS/item1/'\n",
    "file_list1 = glob.glob(newdirectory + '*.txt')\n",
    "#file_list1 = glob.glob(newdirectory + sumdf[\"fName\"])\n",
    "#print(file_list1) # success!\n",
    "\n",
    "filing1 = []\n",
    "n=-1\n",
    "\n",
    "for i in file_list1[0:5]:\n",
    "    print (\"i = \", i) # should get fname\n",
    "    # read the file\n",
    "    with open(i) as f:\n",
    "        #filing = f.read()\n",
    "        filing = f.readlines() # some were one line and some were multi lines...so need combine\n",
    "        print(\"length \", len(filing))\n",
    "        print(n)\n",
    "        if len(filing) > 1:\n",
    "            print(\"long\", n) # this is the (n+1)th file\n",
    "            filing[n : 99999] = [''.join(filing[n : 99999])]\n",
    "            #print(\"this is\",filing[-1]) # seems that the last element is involved\n",
    "        filing1 += filing\n",
    "        print(\"big \", len(filing1))\n",
    "        n = n +1\n",
    "print(\"last\", len(filing))\n",
    "print(len(filing1))  # there can be a big number so the last one is not looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(filing1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in filing1:\n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'filing1', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 32891 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print ('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hli1/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 877 ms, sys: 7.44 ms, total: 884 ms\n",
      "Wall time: 893 ms\n",
      "(8, 1677)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(filing1) #fit the vectorizer to a iterable list of txt\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 65.9 ms, sys: 1.78 ms, total: 67.7 ms\n",
      "Wall time: 74.5 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5                            # doc num > num of clst\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "#uncomment the below to save your model \n",
    "#since I've already run my model I am loading from the pickle\n",
    "joblib.dump(km,  'doc_cluster.pkl')\n",
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shortname should be linked with fname in csv\n",
    "\n",
    "Comps = { 'shortName': sumdf[\"fName\"], 'cluster': clusters}\n",
    "frame = pd.DataFrame(Comps, index = [clusters] , columns = ['shortName', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  shortName  cluster\n",
      "1       NaN        1\n",
      "3       NaN        3\n",
      "0       NaN        0\n",
      "1       NaN        1\n",
      "1       NaN        1\n",
      "2       NaN        2\n",
      "4       NaN        4\n",
      "3       NaN        3\n"
     ]
    }
   ],
   "source": [
    "print(frame) # why do all the short names NA? seem improper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3\n",
       "3    2\n",
       "0    1\n",
       "2    1\n",
       "4    1\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['cluster'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (basic)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

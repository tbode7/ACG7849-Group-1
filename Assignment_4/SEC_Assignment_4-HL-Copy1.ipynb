{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "#### Group 1: Tara Bode, Hankun Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A: Text-based industry classification\n",
    "Cluster the business sections extracted in assignment 3 into 50 clusters (using the cosine similarity) to group firms with similar business descriptions together.\n",
    "\n",
    "Submit your code and the final output csv file, with CIK, conformed end of period, and the cluster number for each business section file.\n",
    "\n",
    "Note: it is likely you will need to tokenize/word count each business section file before reading the next one. (If you try to read all business sections before doing any further processing you will probably run out of memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import html, re\n",
    "from w3lib.html import replace_entities\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from scipy.cluster import  hierarchy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to simply take the.txt files from HiperGator and use the path (r'/blue/acg7849/share/BS/') \n",
    "\n",
    "so we can grab the length of each file for the csv file.\n",
    "\n",
    "Also, we need to use the for loop to cluster the business section files and mimic the class code from yesterday to grab the top 20 common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHeaderVariables(filing):\n",
    "    CIK = filing[ filing.find('CENTRAL INDEX KEY:') + 18  : filing.find('STANDARD') ]\n",
    "    CIK = ''.join(CIK.split())\n",
    "    #\n",
    "    CCN = filing[ filing.find('COMPANY CONFORMED NAME:') + 23 : filing.find('CENTRAL INDEX KEY') ]\n",
    "    CCN = CCN.rstrip().lstrip()\n",
    "    #\n",
    "    CST = filing[ filing.find('CONFORMED SUBMISSION TYPE:') + 26 : filing.find('PUBLIC DOCUMENT COUNT') ]\n",
    "    CST = ''.join(CST.split())\n",
    "    #\n",
    "    CPR = filing[ filing.find('CONFORMED PERIOD OF REPORT:') + 27 : filing.find('FILED AS OF DATE') ]\n",
    "    CPR = ''.join(CPR.split())\n",
    "    #\n",
    "    Length = 'skip'\n",
    "    \n",
    "    sec_info = '{} | {} | {} | {} | {}\\n'.format(CIK, CCN, CST, CPR, Length)\n",
    "    return (sec_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanHTML(raw_html):\n",
    "    cleanr = re.compile('<.*?>', flags=re.DOTALL)\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return replace_entities(cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBusinessSection(filing):\n",
    "    # USE REGEXR\n",
    "    BDS = filing[ filing.find('Item 1. ') : filing.find('Item 1A. ') ]\n",
    "    #print('length_business_section',len(BDS))\n",
    "    #print(BDS)\n",
    "    return BDS\n",
    "    #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv file, with CIK, conformed end of period, and file name (business section file) and the cluster number for each business section file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mydirectory = '/blue/acg7849/share/10Ks'\n",
    "mydirectory = '/blue/acg7849/share/BS/item1'\n",
    "file_list = glob.glob(mydirectory + '/*.txt')\n",
    "# print(file_list) # success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = pd.DataFrame(file_list)\n",
    "obj.to_csv('file_list.csv', index = True) # csv preapared to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "badFiles_raw = []\n",
    "#header vars\n",
    "with open (r'/blue/acg7849/hli1/SEC_Information.txt', 'w') as f:\n",
    "    f.write('CIK|coname|formtype|date|filename|length|cluster\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/blue/acg7849/share/BS/item1/267762.txt\n",
      "66145\n",
      "267762.txt\n",
      "/blue/acg7849/share/BS/item1/280603.txt\n",
      "57117\n",
      "280603.txt\n",
      "/blue/acg7849/share/BS/item1/267435.txt\n",
      "28236\n",
      "bad\n",
      "267435.txt\n",
      "/blue/acg7849/share/BS/item1/275323.txt\n",
      "38468\n",
      "bad\n",
      "275323.txt\n",
      "/blue/acg7849/share/BS/item1/277539.txt\n",
      "36429\n",
      "bad\n",
      "277539.txt\n"
     ]
    }
   ],
   "source": [
    "for i in file_list[0:5]:\n",
    "    print (i)\n",
    "    # read the file\n",
    "    with open(i) as f:\n",
    "        filing = f.read()\n",
    "        \n",
    "    # grab the length of the file\n",
    "    print (len(filing))\n",
    "\n",
    "    # Ignore business section files with less than 1000 characters.\n",
    "    if len(filing) < 50000: # test. should be 1000 here\n",
    "        print ('bad')\n",
    "        badFiles_raw.append (i)\n",
    "    # should end this file and return to next one\n",
    "    \n",
    "    # business info\n",
    "    # get header variables\n",
    "    #first_section = filing[0: filing.find('</DOCUMENT')]\n",
    "   # headervars = str(getHeaderVariables(first_section))\n",
    "    #print(headervars)\n",
    "    \n",
    "    #need to input FileName\n",
    "    shortName = i[i.rfind(\"/\")+1:]\n",
    "    print(shortName)\n",
    "    # tokenize\n",
    "    #word_tokenize (filing[0:10]) # bit different from split - include punctuations\n",
    "#print(badFiles_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/blue/acg7849/share/BS/item1/267435.txt']\n"
     ]
    }
   ],
   "source": [
    "print(badFiles_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in synopses:\n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (basic)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

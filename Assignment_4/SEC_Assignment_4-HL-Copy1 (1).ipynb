{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "#### Group 1: Tara Bode, Hankun Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A: Text-based industry classification\n",
    "Cluster the business sections extracted in assignment 3 into 50 clusters (using the cosine similarity) to group firms with similar business descriptions together.\n",
    "\n",
    "Submit your code and the final output csv file, with CIK, conformed end of period, and the cluster number for each business section file.\n",
    "\n",
    "Note: it is likely you will need to tokenize/word count each business section file before reading the next one. (If you try to read all business sections before doing any further processing you will probably run out of memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import html, re\n",
    "from w3lib.html import replace_entities\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from scipy.cluster import  hierarchy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to simply take the.txt files from HiperGator and use the path (r'/blue/acg7849/share/BS/') \n",
    "\n",
    "so we can grab the length of each file for the csv file.\n",
    "\n",
    "Also, we need to use the for loop to cluster the business section files and mimic the class code from yesterday to grab the top 20 common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHeaderVariables(filing):\n",
    "    CIK = filing[ filing.find('CENTRAL INDEX KEY:') + 18  : filing.find('STANDARD') ]\n",
    "    CIK = ''.join(CIK.split())\n",
    "    #\n",
    "    CCN = filing[ filing.find('COMPANY CONFORMED NAME:') + 23 : filing.find('CENTRAL INDEX KEY') ]\n",
    "    CCN = CCN.rstrip().lstrip()\n",
    "    #\n",
    "    CST = filing[ filing.find('CONFORMED SUBMISSION TYPE:') + 26 : filing.find('PUBLIC DOCUMENT COUNT') ]\n",
    "    CST = ''.join(CST.split())\n",
    "    #\n",
    "    CPR = filing[ filing.find('CONFORMED PERIOD OF REPORT:') + 27 : filing.find('FILED AS OF DATE') ]\n",
    "    CPR = ''.join(CPR.split())\n",
    "    #\n",
    "    Length = 'skip'\n",
    "    \n",
    "    sec_info = '{} | {} | {} | {} | {}\\n'.format(CIK, CCN, CST, CPR, Length)\n",
    "    return (sec_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanHTML(raw_html):\n",
    "    cleanr = re.compile('<.*?>', flags=re.DOTALL)\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return replace_entities(cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBusinessSection(filing):\n",
    "    # USE REGEXR\n",
    "    BDS = filing[ filing.find('Item 1. ') : filing.find('Item 1A. ') ]\n",
    "    #print('length_business_section',len(BDS))\n",
    "    #print(BDS)\n",
    "    return BDS\n",
    "    #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop in A3 - successfully inherited - not preferred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv file, with CIK, conformed end of period, and file name (business section file) and the cluster number for each business section file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydirectory = '/blue/acg7849/share/10Ks'\n",
    "file_list = glob.glob(mydirectory + '/*.txt')\n",
    "#print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write header variables and length of business section to disk\n",
    "with open (r'/blue/acg7849/hli1/SEC_Information.csv', 'w') as f:\n",
    "    f.write('File Name | Central Index Key | Company Name | Submission Type | Period of Report | Length of Business Description\\n')\n",
    "\n",
    "for file_path in file_list[0:10]: # use 10 files to test\n",
    "    # read the file\n",
    "    with open(file_path) as f:\n",
    "        filing = f.read()\n",
    "        \n",
    "    # only use until /DOCUMENT\n",
    "    first_section = filing[0: filing.find('</DOCUMENT')]\n",
    "    \n",
    "    # get header variables\n",
    "    headervars = str(getHeaderVariables(first_section))\n",
    "    \n",
    "    # isolate business section\n",
    "    business_section = str(getBusinessSection(first_section))\n",
    "    \n",
    "    #need to input FileName\n",
    "    shortName = file_path[file_path.rfind(\"/\")+1:]\n",
    "    \n",
    "    # write business section to disk       \n",
    "    with open(r'/blue/acg7849/hli1/item1/'+ shortName, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(cleanHTML(business_section))\n",
    "   \n",
    "    with open (r'/blue/acg7849/hli1/SEC_Information.csv', 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(shortName + ' | ' + cleanHTML(headervars))\n",
    "\n",
    "    #with open (r'/blue/acg7849/tbode/SEC_Information.csv', 'a', encoding=\"utf-8\") as f:\n",
    "     #   f.write(shortName + ' | ' + cleanHTML(headervars) + ' | ' + len(cleanHTML(business_section)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # recommended - use summary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CIK|coname|formtype|date|filename|length\\n', '0001289850|NeuroMetrix, Inc.|10-K|20171231|267762.txt|132616\\n', '0001345016|YELP INC|10-K|20191231|280603.txt|152323\\n', '0001245791|GI DYNAMICS, INC.|10-K|20171231|267435.txt|28236\\n', '0001772177|KURA SUSHI USA, INC.|10-K|20190831|275323.txt|38468\\n']\n"
     ]
    }
   ],
   "source": [
    "# matching filenames\n",
    "\n",
    "with open ('/blue/acg7849/share/BS/summary.txt') as a: \n",
    "    summary = a. readlines()\n",
    "print (summary[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new loop for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdirectory = '/blue/acg7849/share/BS/item1'\n",
    "file_list1 = glob.glob(newdirectory + '/*.txt')\n",
    "#print(file_list1) # success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = pd.DataFrame(file_list)\n",
    "obj.to_csv('file_list.csv', index = True) # csv preapared to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "badFiles_raw = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/blue/acg7849/share/10Ks/267762.txt\n",
      "7858323\n",
      "267762.txt\n",
      "/blue/acg7849/share/10Ks/280603.txt\n",
      "15640951\n",
      "280603.txt\n",
      "/blue/acg7849/share/10Ks/267435.txt\n",
      "9945879\n",
      "267435.txt\n",
      "/blue/acg7849/share/10Ks/275323.txt\n",
      "3221064\n",
      "275323.txt\n",
      "/blue/acg7849/share/10Ks/277539.txt\n",
      "15953715\n",
      "277539.txt\n"
     ]
    }
   ],
   "source": [
    "for i in file_list[0:5]:\n",
    "    print (i)\n",
    "    # read the file\n",
    "    with open(i) as f:\n",
    "        filing = f.read()\n",
    "        \n",
    "    # grab the length of the file\n",
    "    print (len(filing))\n",
    "\n",
    "    # Ignore business section files with less than 1000 characters.\n",
    "    if len(filing) < 50000: # test. should be 1000 here\n",
    "        print ('bad')\n",
    "        badFiles_raw.append (i)\n",
    "    # should end this file and return to next one\n",
    "    # No. of cluster should be 0 or NA\n",
    "\n",
    "    \n",
    "    #need to input FileName\n",
    "    shortName = i[i.rfind(\"/\")+1:]\n",
    "    print(shortName)\n",
    "    # match shortnames to cluster No.\n",
    "    \n",
    "    # tokenize\n",
    "    #word_tokenize (filing[0:10]) # bit different from split - include punctuations\n",
    "#print(badFiles_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(badFiles_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make a list of full texts to do K-means\n",
    "## reference: http://brandonrose.org/clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/blue/acg7849/share/BS/item1/267762.txt\n",
      "1\n",
      "-1\n",
      "/blue/acg7849/share/BS/item1/280603.txt\n",
      "1\n",
      "0\n",
      "/blue/acg7849/share/BS/item1/267435.txt\n",
      "489\n",
      "1\n",
      "2\n",
      "/blue/acg7849/share/BS/item1/275323.txt\n",
      "176\n",
      "2\n",
      "3\n",
      "/blue/acg7849/share/BS/item1/277539.txt\n",
      "1\n",
      "3\n",
      "/blue/acg7849/share/BS/item1/270762.txt\n",
      "2035\n",
      "4\n",
      "5\n",
      "/blue/acg7849/share/BS/item1/274077.txt\n",
      "9\n",
      "5\n",
      "6\n",
      "/blue/acg7849/share/BS/item1/265478.txt\n",
      "1\n",
      "6\n",
      "/blue/acg7849/share/BS/item1/280595.txt\n",
      "1685\n",
      "7\n",
      "8\n",
      "/blue/acg7849/share/BS/item1/283125.txt\n",
      "11\n",
      "8\n",
      "9\n",
      "/blue/acg7849/share/BS/item1/281916.txt\n",
      "4\n",
      "9\n",
      "10\n",
      "/blue/acg7849/share/BS/item1/282859.txt\n",
      "2121\n",
      "10\n",
      "11\n",
      "/blue/acg7849/share/BS/item1/284462.txt\n",
      "1\n",
      "11\n",
      "/blue/acg7849/share/BS/item1/270024.txt\n",
      "1\n",
      "12\n",
      "/blue/acg7849/share/BS/item1/280098.txt\n",
      "747\n",
      "13\n",
      "14\n",
      "/blue/acg7849/share/BS/item1/280672.txt\n",
      "3\n",
      "14\n",
      "15\n",
      "/blue/acg7849/share/BS/item1/274227.txt\n",
      "2\n",
      "15\n",
      "16\n",
      "/blue/acg7849/share/BS/item1/266806.txt\n",
      "1\n",
      "16\n",
      "/blue/acg7849/share/BS/item1/283268.txt\n",
      "1\n",
      "17\n",
      "/blue/acg7849/share/BS/item1/266373.txt\n",
      "649\n",
      "18\n",
      "19\n",
      "/blue/acg7849/share/BS/item1/269026.txt\n",
      "1042\n",
      "19\n",
      "20\n",
      "/blue/acg7849/share/BS/item1/271780.txt\n",
      "1895\n",
      "20\n",
      "21\n",
      "/blue/acg7849/share/BS/item1/282091.txt\n",
      "47\n",
      "21\n",
      "22\n",
      "/blue/acg7849/share/BS/item1/282682.txt\n",
      "392\n",
      "22\n",
      "23\n",
      "/blue/acg7849/share/BS/item1/285179.txt\n",
      "175\n",
      "23\n",
      "24\n",
      "/blue/acg7849/share/BS/item1/280499.txt\n",
      "1019\n",
      "24\n",
      "25\n",
      "/blue/acg7849/share/BS/item1/283423.txt\n",
      "2\n",
      "25\n",
      "26\n",
      "/blue/acg7849/share/BS/item1/285225.txt\n",
      "1\n",
      "26\n",
      "/blue/acg7849/share/BS/item1/276013.txt\n",
      "1054\n",
      "27\n",
      "28\n",
      "/blue/acg7849/share/BS/item1/281615.txt\n",
      "1\n",
      "28\n",
      "/blue/acg7849/share/BS/item1/281230.txt\n",
      "381\n",
      "29\n",
      "30\n",
      "/blue/acg7849/share/BS/item1/277662.txt\n",
      "1\n",
      "30\n",
      "/blue/acg7849/share/BS/item1/272517.txt\n",
      "600\n",
      "31\n",
      "32\n",
      "/blue/acg7849/share/BS/item1/282910.txt\n",
      "1\n",
      "32\n",
      "/blue/acg7849/share/BS/item1/276999.txt\n",
      "4333\n",
      "33\n",
      "34\n",
      "/blue/acg7849/share/BS/item1/283028.txt\n",
      "1\n",
      "34\n",
      "/blue/acg7849/share/BS/item1/273338.txt\n",
      "1305\n",
      "35\n",
      "36\n",
      "/blue/acg7849/share/BS/item1/271856.txt\n",
      "771\n",
      "36\n",
      "37\n",
      "/blue/acg7849/share/BS/item1/281172.txt\n",
      "1\n",
      "37\n",
      "/blue/acg7849/share/BS/item1/272884.txt\n",
      "5\n",
      "38\n",
      "39\n",
      "/blue/acg7849/share/BS/item1/269380.txt\n",
      "6\n",
      "39\n",
      "40\n",
      "/blue/acg7849/share/BS/item1/285753.txt\n",
      "1\n",
      "40\n",
      "/blue/acg7849/share/BS/item1/274327.txt\n",
      "1\n",
      "41\n",
      "/blue/acg7849/share/BS/item1/268590.txt\n",
      "104\n",
      "42\n",
      "43\n",
      "/blue/acg7849/share/BS/item1/278199.txt\n",
      "399\n",
      "43\n",
      "44\n",
      "/blue/acg7849/share/BS/item1/269972.txt\n",
      "9\n",
      "44\n",
      "45\n",
      "/blue/acg7849/share/BS/item1/285147.txt\n",
      "1\n",
      "45\n",
      "/blue/acg7849/share/BS/item1/285320.txt\n",
      "967\n",
      "46\n",
      "47\n",
      "/blue/acg7849/share/BS/item1/276540.txt\n",
      "213\n",
      "47\n",
      "48\n",
      "/blue/acg7849/share/BS/item1/273038.txt\n",
      "1\n",
      "48\n",
      "672\n"
     ]
    }
   ],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "\n",
    "newdirectory = '/blue/acg7849/share/BS/item1'\n",
    "file_list1 = glob.glob(newdirectory + '/*.txt')\n",
    "#print(file_list1) # success!\n",
    "\n",
    "filing1 = []\n",
    "n=-1\n",
    "\n",
    "for i in file_list1[0:50]:\n",
    "    print (i)\n",
    "    # read the file\n",
    "    with open(i) as f:\n",
    "        #filing = f.read()\n",
    "        filing = f.readlines() # some were one line and some were multi lines...so need combine\n",
    "        print(\"num of elem:\"len(filing))\n",
    "        print(n)\n",
    "        n = n +1\n",
    "        if len(filing) > 1:\n",
    "            print(n) # this is the (n-1)th file\n",
    "            filing[n : 99999] = [''.join(filing[n : 99999])]\n",
    "            #print(\"this is\",filing[-1]) # seems that the last element is involved\n",
    "        filing1 += filing\n",
    "print(len(filing1))  # there can be a big number so the last one is not looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(filing1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in filing1:\n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'filing1', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 72841 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print ('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hli1/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.77 s, sys: 4.22 ms, total: 1.78 s\n",
      "Wall time: 1.79 s\n",
      "(49, 170)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(filing1) #fit the vectorizer to a iterable list of txt\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 62 ms, sys: 0 ns, total: 62 ms\n",
      "Wall time: 63.6 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5                            # doc num > num of clst\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "#uncomment the below to save your model \n",
    "#since I've already run my model I am loading from the pickle\n",
    "joblib.dump(km,  'doc_cluster.pkl')\n",
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "Comps = { 'shortName': shortName, 'cluster': clusters}\n",
    "frame = pd.DataFrame(Comps, index = [clusters] , columns = ['shortName', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    shortName  cluster\n",
      "1  277539.txt        1\n",
      "1  277539.txt        1\n",
      "2  277539.txt        2\n",
      "0  277539.txt        0\n",
      "1  277539.txt        1\n",
      "0  277539.txt        0\n",
      "3  277539.txt        3\n",
      "0  277539.txt        0\n",
      "0  277539.txt        0\n",
      "1  277539.txt        1\n",
      "0  277539.txt        0\n",
      "1  277539.txt        1\n",
      "2  277539.txt        2\n",
      "0  277539.txt        0\n",
      "0  277539.txt        0\n",
      "0  277539.txt        0\n",
      "0  277539.txt        0\n",
      "1  277539.txt        1\n",
      "0  277539.txt        0\n",
      "2  277539.txt        2\n",
      "0  277539.txt        0\n",
      "0  277539.txt        0\n",
      "0  277539.txt        0\n",
      "0  277539.txt        0\n",
      "0  277539.txt        0\n",
      "0  277539.txt        0\n",
      "0  277539.txt        0\n",
      "1  277539.txt        1\n",
      "2  277539.txt        2\n",
      "3  277539.txt        3\n",
      "0  277539.txt        0\n",
      "0  277539.txt        0\n",
      "2  277539.txt        2\n",
      "0  277539.txt        0\n",
      "0  277539.txt        0\n",
      "0  277539.txt        0\n",
      "1  277539.txt        1\n",
      "0  277539.txt        0\n",
      "4  277539.txt        4\n",
      "4  277539.txt        4\n",
      "4  277539.txt        4\n",
      "4  277539.txt        4\n",
      "4  277539.txt        4\n",
      "4  277539.txt        4\n",
      "4  277539.txt        4\n",
      "4  277539.txt        4\n",
      "4  277539.txt        4\n",
      "4  277539.txt        4\n",
      "0  277539.txt        0\n"
     ]
    }
   ],
   "source": [
    "print(frame) # why do all the short names same? seem improper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    24\n",
       "4    10\n",
       "1     8\n",
       "2     5\n",
       "3     2\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which cluster are the comps in accordingly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (basic)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

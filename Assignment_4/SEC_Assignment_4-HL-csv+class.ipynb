{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5 inclass\n",
    "#### Group 1: Tara Bode, Hankun Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A: Text-based industry classification\n",
    "Cluster the business sections extracted in assignment 3 into 50 clusters (using the cosine similarity) to group firms with similar business descriptions together.\n",
    "\n",
    "Submit your code and the final output csv file, with CIK, conformed end of period, and the cluster number for each business section file.\n",
    "\n",
    "Note: it is likely you will need to tokenize/word count each business section file before reading the next one. (If you try to read all business sections before doing any further processing you will probably run out of memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import html, re\n",
    "from w3lib.html import replace_entities\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from scipy.cluster import  hierarchy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to simply take the.txt files from HiperGator and use the path (r'/blue/acg7849/share/BS/') \n",
    "\n",
    "so we can grab the length of each file for the csv file.\n",
    "\n",
    "Also, we need to use the for loop to cluster the business section files and mimic the class code from yesterday to grab the top 20 common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv file, with CIK, conformed end of period, and file name (business section file) and the cluster number for each business section file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # recommended - use summary files - got initial csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CIK|coname|formtype|date|filename|length\\n', '0001289850|NeuroMetrix, Inc.|10-K|20171231|267762.txt|132616\\n', '0001345016|YELP INC|10-K|20191231|280603.txt|152323\\n', '0001245791|GI DYNAMICS, INC.|10-K|20171231|267435.txt|28236\\n', '0001772177|KURA SUSHI USA, INC.|10-K|20190831|275323.txt|38468\\n']\n",
      "          CIK                coName formtype      date       fName    length\n",
      "1  0001289850     NeuroMetrix, Inc.     10-K  20171231  267762.txt  132616\\n\n",
      "2  0001345016              YELP INC     10-K  20191231  280603.txt  152323\\n\n",
      "3  0001245791     GI DYNAMICS, INC.     10-K  20171231  267435.txt   28236\\n\n",
      "4  0001772177  KURA SUSHI USA, INC.     10-K  20190831  275323.txt   38468\\n\n",
      "5  0000832428       E.W. SCRIPPS Co     10-K  20181231  277539.txt  197843\\n\n"
     ]
    }
   ],
   "source": [
    "# matching filenames\n",
    "\n",
    "with open ('/blue/acg7849/share/BS/summary.txt') as a: \n",
    "    summary = a. readlines()\n",
    "print (summary[0:5])\n",
    "\n",
    "sumdf = pd.DataFrame (summary)\n",
    "#sumdf.str.split(pat=\"|\")\n",
    "\n",
    "\n",
    "new = sumdf[0].str.split(\"|\", expand = True)\n",
    "sumdf[\"CIK\"]= new[0]\n",
    "sumdf[\"coName\"]= new[1]\n",
    "sumdf[\"formtype\"]= new[2]\n",
    "sumdf[\"date\"]= new[3]\n",
    "sumdf[\"fName\"]= new[4]\n",
    "sumdf[\"length\"]= new[5]\n",
    "sumdf = sumdf.drop(columns=[0])\n",
    "sumdf = sumdf.drop([0])\n",
    "\n",
    "print (sumdf[0:5])                        \n",
    "#Initialize csv - no clusters\n",
    "#sumdf.to_csv('ASG4.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# proceed for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdirectory = '/blue/acg7849/share/BS/item1'\n",
    "#file_list1 = glob.glob(newdirectory + '/*.txt')\n",
    "#print(file_list1) # success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          CIK                coName formtype      date       fName    length  \\\n",
      "1  0001289850     NeuroMetrix, Inc.     10-K  20171231  267762.txt  132616\\n   \n",
      "2  0001345016              YELP INC     10-K  20191231  280603.txt  152323\\n   \n",
      "3  0001245791     GI DYNAMICS, INC.     10-K  20171231  267435.txt   28236\\n   \n",
      "4  0001772177  KURA SUSHI USA, INC.     10-K  20190831  275323.txt   38468\\n   \n",
      "5  0000832428       E.W. SCRIPPS Co     10-K  20181231  277539.txt  197843\\n   \n",
      "\n",
      "                                 full_link  \n",
      "1  /blue/acg7849/share/BS/item1/267762.txt  \n",
      "2  /blue/acg7849/share/BS/item1/280603.txt  \n",
      "3  /blue/acg7849/share/BS/item1/267435.txt  \n",
      "4  /blue/acg7849/share/BS/item1/275323.txt  \n",
      "5  /blue/acg7849/share/BS/item1/277539.txt  \n"
     ]
    }
   ],
   "source": [
    "sumdf[\"full_link\"]= newdirectory +'/'+ sumdf[\"fName\"]\n",
    "print (sumdf[0:5]) # found new set of links!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/blue/acg7849/share/BS/item1/267762.txt', '/blue/acg7849/share/BS/item1/280603.txt', '/blue/acg7849/share/BS/item1/267435.txt', '/blue/acg7849/share/BS/item1/275323.txt', '/blue/acg7849/share/BS/item1/277539.txt']\n"
     ]
    }
   ],
   "source": [
    "listoflink = list(sumdf[\"full_link\"]) # in case we need a list\n",
    "print (listoflink[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/blue/acg7849/share/BS/item1/267762.txt\n",
      "66145\n",
      "/blue/acg7849/share/BS/item1/280603.txt\n",
      "57117\n",
      "/blue/acg7849/share/BS/item1/267435.txt\n",
      "28236\n",
      "/blue/acg7849/share/BS/item1/275323.txt\n",
      "38468\n",
      "/blue/acg7849/share/BS/item1/277539.txt\n",
      "36429\n"
     ]
    }
   ],
   "source": [
    "for i in listoflink[0:5]:\n",
    "    print (i)\n",
    "    # read the file\n",
    "    with open(i) as f:\n",
    "        filing = f.read()\n",
    "        \n",
    "    # grab the length of the file\n",
    "    print (len(filing))           # read file OK, can proceed to cluster\n",
    "\n",
    "    #shortName = i[i.rfind(\"/\")+1:]\n",
    "    #print(shortName)\n",
    "    \n",
    "    # match shortnames to cluster No.\n",
    "    \n",
    "    # tokenize\n",
    "    #word_tokenize (filing[0:10]) # bit different from split - include punctuations\n",
    "#print(badFiles_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = vectorizer.fit_transform(generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# piazza advices \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "documents = (\n",
    "\"The sky is blue\",\n",
    "\"The sun is bright\",\n",
    "\"The sun in sky is bright\",\n",
    "\"We can see the shining sun, bright sun\"\n",
    ")\n",
    "def docGen():\n",
    "    yield documents[0]\n",
    "    yield documents[1]\n",
    "    yield documents[2]\n",
    "    yield documents[3]\n",
    "\n",
    "tfidf = vectorizer.fit_transform(docGen() )\n",
    "print ('size tfidf matrix (#documents, #unique words):', tfidf.shape)\n",
    "\n",
    "print('unique words', vectorizer.get_feature_names() )\n",
    "\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator\n",
    "class MyCommentLetters(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        #for fname in os.listdir(self.dirname)[0:2]:\n",
    "        for fname in os.listdir(self.dirname)[0:200]:\n",
    "            \n",
    "            for line in open(os.path.join(self.dirname, fname), encoding='utf-8'):\n",
    "                # cleaner (but slower)\n",
    "                yield [ x for x in word_tokenize(line) if x.isalpha() and x.lower() not in stopWords and x not in punc ]\n",
    "                # faster (but not so clean)\n",
    "                #yield line.lower().split()\n",
    "                \n",
    "import gensim\n",
    "# min-count is how minimum count of each word, default is 5\n",
    "# workers: #cores (may need cython to be installed for this to have an effect)\n",
    "model = gensim.models.Word2Vec(wordLists, min_count=2, workers=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (basic)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

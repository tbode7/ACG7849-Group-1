{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment letters\n",
    "\n",
    "Read the comment letters, compute doc2vec vector (for each letter) and group similar letters in 10 buckets\n",
    "\n",
    "See https://towardsdatascience.com/automatic-topic-clustering-using-doc2vec-e1cea88449c\n",
    "and https://github.com/olafmaas/hackdelft/blob/master/hackdelft/doc2vec/generate_json.py            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make df and iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import html, re\n",
    "from w3lib.html import replace_entities\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from scipy.cluster import  hierarchy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CIK|coname|formtype|date|filename|length\\n', '0001289850|NeuroMetrix, Inc.|10-K|20171231|267762.txt|132616\\n', '0001345016|YELP INC|10-K|20191231|280603.txt|152323\\n', '0001245791|GI DYNAMICS, INC.|10-K|20171231|267435.txt|28236\\n', '0001772177|KURA SUSHI USA, INC.|10-K|20190831|275323.txt|38468\\n']\n"
     ]
    }
   ],
   "source": [
    "# matching filenames\n",
    "\n",
    "with open ('/blue/acg7849/share/BS/summary.txt') as a: \n",
    "    summary = a. readlines()\n",
    "print (summary[0:5])\n",
    "\n",
    "sumdf = pd.DataFrame (summary)\n",
    "#sumdf.str.split(pat=\"|\")\n",
    "\n",
    "new = sumdf[0].str.split(\"|\", expand = True)\n",
    "sumdf[\"CIK\"]= new[0]\n",
    "sumdf[\"coName\"]= new[1]\n",
    "sumdf[\"formtype\"]= new[2]\n",
    "sumdf[\"date\"]= new[3]\n",
    "sumdf[\"fName\"]= new[4]\n",
    "sumdf[\"length\"]= new[5]\n",
    "sumdf = sumdf.drop(columns=[0])\n",
    "sumdf = sumdf.drop([0])\n",
    "\n",
    "#print (sumdf[0:15])                        \n",
    "#Initialize csv - no clusters\n",
    "#sumdf.to_csv('ASG4.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/blue/acg7849/hli1/item1/267762.txt', '/blue/acg7849/hli1/item1/280603.txt', '/blue/acg7849/hli1/item1/267435.txt', '/blue/acg7849/hli1/item1/275323.txt', '/blue/acg7849/hli1/item1/277539.txt']\n"
     ]
    }
   ],
   "source": [
    "sumdf[\"length\"]= pd.to_numeric(sumdf[\"length\"])\n",
    "# filter out all the things with <1000 length \n",
    "sumdf = sumdf.loc[sumdf[\"length\"] >= 1000]\n",
    "#print (sumdf[0:15])     # seems filtering success! \n",
    "\n",
    "sumdf_bk  = sumdf  # make another file as backup\n",
    "sumdf_test  = sumdf[:100]  # test length = 100\n",
    "\n",
    "mydirectory = '/blue/acg7849/hli1/item1'\n",
    "newdirectory = '/blue/acg7849/share/BS/item1'\n",
    "\n",
    "sumdf[\"full_link\"]= newdirectory +'/'+ sumdf[\"fName\"]\n",
    "#print (sumdf[0:13]) \n",
    "listoflink = list(sumdf[\"full_link\"]) # in case we need a list\n",
    "listofmylink = list(mydirectory +'/'+ sumdf[\"fName\"])\n",
    "\n",
    "print (listofmylink[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/jupyterhub/1.1.0/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os, string, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english') )\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some punctuation to string.punctuation\n",
    "punc = string.punctuation + '“”'\n",
    "\n",
    "# documents get tagged by an index (nu,mber), while filenames have different numbers\n",
    "# keep track of this\n",
    "fileIdToIndex = {} # given a fileId -> tag\n",
    "indexToFileId=[] # given a tag -> fileId\n",
    "\n",
    "class MyCommentLetters(object):\n",
    "    def __init__(self, dirname, tokens_only = False):\n",
    "        self.dirname = dirname\n",
    "        self.tokens_only = tokens_only\n",
    " \n",
    "    def __iter__(self):\n",
    "        for i, fname in enumerate(os.listdir(self.dirname)[0:300]):  # here are the number of files at max?\n",
    "        #for fname in os.listdir(self.dirname):\n",
    "            with open( os.path.join(self.dirname, fname), encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # grab id from filename\n",
    "            myCounter = int (  re.findall(r'(\\d*)\\.txt', fname)[0] )\n",
    "            # update \n",
    "            fileIdToIndex [ myCounter] = i\n",
    "            indexToFileId.append( myCounter)\n",
    "            #print('fname', fname, 'tag', myCounter)\n",
    "            file_tokens = [x for x in word_tokenize(content) if x.isalpha() and x.lower() not in stopWords and x not in string.punctuation]\n",
    "            \n",
    "            if self.tokens_only == True:\n",
    "                yield file_tokens\n",
    "            else:\n",
    "                yield TaggedDocument(words=file_tokens, tags=[i] )                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/blue/acg7849/hli1/item1/267762.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-acc47aa4f382>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mmine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/blue/acg7849/hli1/item1/267762.txt'"
     ]
    }
   ],
   "source": [
    "# make a file address in my HPG directory, write from listoflink\n",
    "# copy and paste\n",
    "#listoflink\n",
    "import shutil\n",
    "\n",
    "# open one file and then write it to directory\n",
    "\n",
    "for i in listoflink:\n",
    "    for p in listofmylink:\n",
    "        with open( i, \"r\", encoding='utf-8') as f:\n",
    "            output = f.read()\n",
    "        with open( p, \"r\", encoding='utf-8') as f:\n",
    "            mine = f.write()\n",
    "\n",
    "            myfile = open(filename, 'w')\n",
    "            myfile.writelines(s)\n",
    "            myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hipergator\n",
    "wordLists = MyCommentLetters(newdirectory) # a memory-friendly iterator, should iterate through a file address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model, build vocabulary\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=2, epochs=10)\n",
    "model.build_vocab(wordLists)   # list index out of range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fileIdToIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267106"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexToFileId[ 290  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "myText = '10-K'\n",
    "print( myText.isalpha() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train it\n",
    "model.train(wordLists, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because we are using a generator, the files are no longer in memory\n",
    "# Hipergator\n",
    "def tokenizeFile(file_id):\n",
    "    with open( r'/blue/acg7849/share/BS/item1/'+str(file_id)+'.txt', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    return ([x for x in word_tokenize(content) if x.isalpha() and x.lower() not in stopWords and x not in string.punctuation] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.196132  , -5.127264  , -3.1189978 ,  0.40635327, -0.6051856 ,\n",
       "       -1.0871681 , -3.1914911 , -4.4955297 , -5.23893   , -1.9187999 ,\n",
       "        1.250936  ,  2.6159403 , -4.534772  ,  3.9957583 , -3.7161956 ,\n",
       "       -0.5586631 ,  1.2324674 , -0.53410465, -3.0927725 , -2.7281384 ,\n",
       "       -1.1377373 , -0.4621333 ,  0.9403881 , -0.4959484 ,  4.9005747 ,\n",
       "       -0.73659253,  2.1769545 ,  3.2041395 , -0.71954346,  0.33247924,\n",
       "       -2.496773  ,  1.8453736 , -2.8237495 , -0.9597754 ,  1.0134658 ,\n",
       "       -2.6949205 ,  1.4732404 ,  0.55403125, -0.44319275,  0.6034303 ,\n",
       "       -2.5383945 ,  1.1881021 ,  0.5423492 , -1.0921609 ,  0.6440071 ,\n",
       "        0.6952304 , -3.2677782 ,  1.213193  ,  1.7657232 , -1.062602  ,\n",
       "       -0.41510707, -0.28982353, -2.104023  , -3.7741446 , -0.2790829 ,\n",
       "        2.8520324 ,  0.33070278,  2.0624163 ,  2.5022595 ,  3.0887873 ,\n",
       "       -1.5978309 ,  1.5314784 ,  0.06153132, -0.3575046 , -0.6215774 ,\n",
       "        2.5407457 , -0.5688942 ,  0.6047784 ,  1.4386171 ,  0.84653944,\n",
       "       -1.7520938 , -1.3894063 ,  0.54391474, -3.9076483 , -3.3985078 ,\n",
       "        1.9908237 , -3.5902417 , -4.1774025 ,  0.8185184 , -0.8198814 ,\n",
       "       -2.9168675 , -2.2511184 ,  0.6809359 ,  0.13825035,  5.413243  ,\n",
       "       -1.8515801 , -1.2021282 , -3.4429805 ,  0.6153959 , -0.20543733,\n",
       "       -0.08807569, -1.6135696 , -2.5935602 , -1.7297766 , -2.2409759 ,\n",
       "        0.09362829, -1.0513319 ,  1.4646822 ,  1.2124783 ,  3.7986414 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizeFile(267762)\n",
    "model.infer_vector( t )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 0.6333470344543457)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#t = tokenizeFile(1)\n",
    "inferred_vector = model.infer_vector( t )\n",
    "# dv is short for docvecs\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "#sims = model.dv.most_similar([inferred_vector], topn=4)\n",
    "sims[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-e11ffd9e85f7>:2: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  similar_doc = model.docvecs.most_similar(1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(171, 0.804776132106781),\n",
       " (91, 0.787969172000885),\n",
       " (299, 0.7870095372200012),\n",
       " (112, 0.7683703899383545),\n",
       " (130, 0.7682340145111084),\n",
       " (271, 0.7531971335411072),\n",
       " (222, 0.7139574885368347),\n",
       " (267, 0.6966142058372498),\n",
       " (29, 0.6955546736717224),\n",
       " (119, 0.6954190135002136)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# letter with filename 1.txt is the first letter, so tag is 0\n",
    "similar_doc = model.docvecs.most_similar(1)\n",
    "similar_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents 300\n",
      "model.docvecs 300\n"
     ]
    }
   ],
   "source": [
    "print('number of documents', model.corpus_count)\n",
    "print('model.docvecs', len(model.dv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hipergator\n",
    "# reread the files, and get the vector for each file\n",
    "# feed vector into k-means algorithm to make clusters\n",
    "wordLists = MyCommentLetters(r'/blue/acg7849/share/BS/item1/', tokens_only = True) # a memory-friendly iterator\n",
    "vectors = [ model.infer_vector( w ) for w in wordLists]\n",
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.1704612 , -5.130845  , -3.15631   ,  0.44030204, -0.4878984 ,\n",
       "       -1.1170868 , -3.2990344 , -4.6746206 , -5.4853024 , -2.2243352 ,\n",
       "        1.3230593 ,  2.7618692 , -4.4890447 ,  4.0193677 , -3.94044   ,\n",
       "       -0.39660156,  1.2420162 , -0.41996512, -3.2657363 , -2.8233726 ,\n",
       "       -1.0030143 , -0.5569476 ,  0.93863446, -0.45166194,  4.879965  ,\n",
       "       -0.9073644 ,  2.0911472 ,  3.250878  , -0.93150634,  0.268633  ,\n",
       "       -2.8722794 ,  1.7253896 , -2.902139  , -0.9727945 ,  1.1975088 ,\n",
       "       -2.748817  ,  1.5173533 ,  0.6490475 , -0.525698  ,  0.6934823 ,\n",
       "       -2.347691  ,  1.1805925 ,  0.6909716 , -1.3081062 ,  0.65291697,\n",
       "        0.6771697 , -2.9531481 ,  1.1721892 ,  1.6826416 , -0.87920207,\n",
       "       -0.44763726, -0.20184833, -2.0522463 , -3.5810897 , -0.04629781,\n",
       "        2.9264932 ,  0.4587552 ,  2.249673  ,  2.5155492 ,  2.9339454 ,\n",
       "       -1.612512  ,  1.4867091 ,  0.24880655, -0.44831407, -0.5424832 ,\n",
       "        2.1672578 , -0.496822  ,  0.53103   ,  1.5260354 ,  0.79300493,\n",
       "       -1.7012335 , -1.3342304 ,  0.67637044, -3.7928195 , -3.526197  ,\n",
       "        2.1810725 , -3.510624  , -4.3501997 ,  0.7358212 , -0.6905846 ,\n",
       "       -2.8449216 , -2.2852275 ,  0.84784293,  0.0454467 ,  5.2730556 ,\n",
       "       -1.8873664 , -1.2645856 , -3.56567   ,  0.83983403, -0.33530468,\n",
       "       -0.40318093, -1.6261818 , -2.6914096 , -1.8654037 , -2.136873  ,\n",
       "        0.2662227 , -0.7086854 ,  1.6461246 ,  1.1526759 ,  3.8123105 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.cluster import KMeansClusterer\n",
    "num_clusters = 50\n",
    "kclusterer = KMeansClusterer(num_clusters, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "assigned_clusters = kclusterer.cluster(vectors, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 46, 33, 18, 49, 25, 34, 28, 29, 46, 42, 1, 24, 39, 14, 44, 41, 12, 32, 21]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assigned_clusters[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({27: 22, 17: 17, 43: 17, 41: 15, 21: 12, 42: 9, 47: 9, 46: 8, 34: 8, 39: 8, 40: 8, 35: 8, 6: 8, 28: 7, 10: 7, 20: 7, 33: 6, 1: 6, 16: 6, 3: 6, 45: 6, 48: 6, 18: 5, 49: 5, 14: 5, 44: 5, 31: 5, 13: 5, 19: 5, 15: 5, 38: 5, 9: 4, 29: 4, 12: 4, 2: 4, 4: 4, 25: 3, 24: 3, 32: 3, 0: 3, 11: 3, 37: 3, 23: 2, 22: 2, 7: 2, 36: 1, 26: 1, 30: 1, 8: 1, 5: 1})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "print(collections.Counter(assigned_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class question\n",
    "\n",
    "For each cluster, what are the 20 most frequent words (excluding stop words, excluding punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IndexError: list index out of range  (with 300 files read)\n",
    "\n",
    "#keep track of fileIds for letters in different clusters\n",
    "ids = [ [] for i in range(10) ]\n",
    "for i, clust in enumerate(assigned_clusters):\n",
    "    # look up the filename for i (which is the tag), then add it to the list for that cluster\n",
    "    ids[ clust - 1 ].append( indexToFileId[ i ]  )     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[266806,\n",
       "  276013,\n",
       "  278199,\n",
       "  285320,\n",
       "  275923,\n",
       "  284230,\n",
       "  278425,\n",
       "  275820,\n",
       "  281337,\n",
       "  268181,\n",
       "  279912,\n",
       "  275007],\n",
       " [285179,\n",
       "  280499,\n",
       "  281703,\n",
       "  272933,\n",
       "  265514,\n",
       "  266283,\n",
       "  271008,\n",
       "  265393,\n",
       "  277411,\n",
       "  271174,\n",
       "  283258,\n",
       "  277446,\n",
       "  269522,\n",
       "  271690,\n",
       "  269881,\n",
       "  274401,\n",
       "  279101,\n",
       "  279597]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first two clusters\n",
    "ids[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "# now read the comment letters for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '266806.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-951e29ffa9cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclust\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfile_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopWords\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '266806.txt'"
     ]
    }
   ],
   "source": [
    "for i, clust in enumerate(ids):\n",
    "    content = \"\"\n",
    "    for file in clust:\n",
    "        with open( str(file) + \".txt\", encoding='utf-8') as f:# No such file or directory: '266806.txt'\n",
    "            content += f.read()\n",
    "    file_tokens = [x.lower() for x in word_tokenize(content) if x.isalpha() and x.lower() not in stopWords and x not in string.punctuation]\n",
    "    # now we can use nltk functions on the text\n",
    "    fdist = FreqDist(file_tokens)\n",
    "    print ('cluster', i+ 1, 'most common words in this letter', fdist.most_common(5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (basic)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
